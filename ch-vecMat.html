<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="10 Vector and Matrix | Project XXII" />
<meta property="og:type" content="book" />





<meta name="author" content="Zhengyuan Gao" />

<meta name="date" content="2021-06-01" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="10 Vector and Matrix | Project XXII">

<title>10 Vector and Matrix | Project XXII</title>

<link href="libs/tufte-css/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/htmlwidgets/htmlwidgets.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding/datatables.js"></script>
<link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk/js/crosstalk.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { background-color: #f8f8f8; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>



</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Project XXII<p><p class="author">Zhengyuan Gao</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BA; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Preface</a>
<a href="part-i-inference-based-upon-logic-and-logical-computation.html">PART I: Inference Based upon Logic and Logical Computation</a>
<a href="sub-logic.html"><span class="toc-section-number">1</span> Logic</a>
<a href="sub-set-theory.html"><span class="toc-section-number">2</span> Set Theory</a>
<a href="sub-axioms.html"><span class="toc-section-number">3</span> Axioms</a>
<a href="sub-inferknow.html"><span class="toc-section-number">4</span> Inference and Knowledge</a>
<a href="sub-incomplete.html"><span class="toc-section-number">5</span> Incompleteness</a>
<a href="part-ii-infinitesimal-changes-and-their-consequences.html">PART II: Infinitesimal Changes and their Consequences</a>
<a href="sub-continuity.html"><span class="toc-section-number">6</span> Continuity</a>
<a href="sub-calculus.html"><span class="toc-section-number">7</span> Calculus</a>
<a href="ch-DE.html"><span class="toc-section-number">8</span> Differential Equations</a>
<a href="ch-CalUn.html"><span class="toc-section-number">9</span> Calculus under Uncertainty</a>
<a href="part-iii-emergence-of-abstract-interactions.html">PART III: Emergence of Abstract Interactions</a>
<a id="active-page" href="ch-vecMat.html"><span class="toc-section-number">10</span> Vector and Matrix</a><ul class="toc-sections">
<li class="toc"><a href="#sub:vec"> Vector</a></li>
<li class="toc"><a href="#sub:linearity"> Example: Linearity</a></li>
<li class="toc"><a href="#sub:matrix"> Matrix</a></li>
<li class="toc"><a href="#sub:linearSys"> Example: Linear Systems</a></li>
<li class="toc"><a href="#sub:symbolism"> Miscellaneous: Mythology and Symbolism</a></li>
</ul>
<a href="ch-MatComp.html"><span class="toc-section-number">11</span> Matrix Computation</a>
<a href="ch-eigen.html"><span class="toc-section-number">12</span> Eigenvalues and Eigenvectors</a>
<a href="ch-UnMulti.html"><span class="toc-section-number">13</span> Uncertainty in Multiple Dimensions</a>
<a href="part-iv-three-masons-to-illuminate-the-dual-world.html">PART IV: Three Masons to Illuminate the Dual World</a>
<a href="ch-representation.html"><span class="toc-section-number">14</span> Representation</a>
<a href="ch-optApp.html"><span class="toc-section-number">15</span> Optimum Approximation</a>
<a href="ch-randomization.html"><span class="toc-section-number">16</span> Randomization</a>
<a href="part-v-complex-ensembles.html">PART V: Complex Ensembles</a>
<a href="ch-complexAn.html"><span class="toc-section-number">17</span> Complex Analysis</a>
<a href="ch-harmonicAn.html"><span class="toc-section-number">18</span> Harmonic Analysis</a>
<a href="bibliography.html">Bibliography</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="ch:vecMat" class="section level1">
<h1>
<span class="header-section-number">10</span> Vector and Matrix</h1>
<p>We don’t add an apple to orange because they are different fruits. Similarly, the values of two different <strong>unknowns</strong> <span class="math inline">\(x_{1}\)</span> and <span class="math inline">\(x_{2}\)</span> from a linear system
<span class="math display" id="eq:sem-1">\[\begin{equation}
\begin{cases}
0.3x_{2}+0.6x_{1} &amp; =1,\\
0.6x_{2}-0.3x_{1} &amp; =0.
\end{cases}
\tag{10.1} 
\end{equation}\]</span>
should be treated separately as we did it for the <a href="sub-set-theory.html#sub:order">ordered list</a> <span class="math inline">\((x_{1},x_{2})\in\mathbb{R}^{2}\)</span>. The solution <span class="math inline">\((4/3,\,2/3)\)</span> can be viewed as the point on the plane where two equations intersect (figure <a href="ch-vecMat.html#fig:LinearSys">10.1</a>). The notation of <span class="math inline">\((x_{1},x_{2})\)</span> is free to express any point on the plane. The introduction of a great workable literal symbolism was a significant advance in mathematics. Descartes illustrated his entire scheme of geometry on the Cartesian system of coordinates. This illustration connected algebra to classical geometry.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:LinearSys"></span>
<img src="fig/Part3/LinearSys.png" alt="System of two equations" width="100%"><!--
<p class="caption marginnote">-->Figure 10.1: System of two equations<!--</p>-->
<!--</div>--></span>
</p>
<p>However, the geometrical approach becomes impractical when the linear system includes more than three <strong>knowns</strong>. Graphing in four (or more) dimensions on a flat sheet of paper does not lead to accurate answers. But Descartes’ achievement inspired Leibniz’s dream of <a href="">symbolism</a> for all of the human thought, in which all arguments about truth or falsehood could be resolved by the computations of symbols. Leibniz argued that such <a href="">symbolism</a> would relieve the imagination.</p>
<p>To initiate our journey to the imaginary world of symbols, let’s take a closer look at the system <a href="ch-vecMat.html#eq:sem-1">(10.1)</a>. The useful information of this system is stored by the coefficients on the left-hand side of the equality and by the output values on the right-hand side. The following two tables can compactly list all the necessary information: <span class="math display">\[\left[\begin{array}{cc}
0.3 &amp; 0.6\\
0.6 &amp; -0.3
\end{array}\right],\:\left[\begin{array}{c}
1\\
0
\end{array}\right].\]</span>
There is no need to write out all the equal signs or plus signs. These rectangular tables of numbers are handy in representing the system of equations. The first rectangular table of numbers represents a symbol called the <a href="ch-vecMat.html#sub:matrix">matrix</a>, and the second represents a symbol called the <a href="ch-vecMat.html#sub:vec">vector</a>. They are the primary objects for studying a general linear system with an arbitrary number of unknowns. By relieving our imagination, we can reduce high-dimensional thought processes to some easily mastered manipulations of symbols.</p>
<div id="sub:vec" class="section level2">
<h2>
<span class="header-section-number">10.1</span> Vector</h2>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:Vector"></span>
<img src="fig/Part3/Vector.gif" alt="Vector" width="100%"><!--
<p class="caption marginnote">-->Figure 10.2: Vector<!--</p>-->
<!--</div>--></span>
</p>
<p>If we look at the point <span class="math inline">\((a_{1},a_{2})\in \mathbb{R}^{2}\)</span> of the plane in figure <a href="ch-vecMat.html#fig:Vector">10.2</a>, there is a line going from the origin to it. Two characteristics of this point, its length and its direction, have been automatically stored in this <a href="sub-set-theory.html#sub:order">ordered list</a>. If we scale this line by <span class="math inline">\(c\)</span>, then the point will move to <span class="math inline">\((ca_{1},ca_{2})\)</span>; and if we add another <span class="math inline">\((b_{1},b_{2})\)</span> to this point, there will be a new <a href="sub-set-theory.html#sub:order">ordered list</a> <span class="math inline">\((a_{1}+b_{1},\, a_{2}+b_{2})\)</span>.<label for="tufte-sn-152" class="margin-toggle sidenote-number">152</label><input type="checkbox" id="tufte-sn-152" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">152</span> The order does not matter for the addition as <span class="math inline">\((a_{1}+b_{1},\, a_{2}+b_{2})\)</span> and <span class="math inline">\((b_{1}+a_{1},\, b_{2}+a_{2})\)</span> are the same.</span></p>
<p>Let’s relieve our imagination to an ordered list of <span class="math inline">\(n\)</span> elements. Even it is hard to imagine what is an <span class="math inline">\(n\)</span>-dimensional space, we may imagine that the direction, length, simple operations such as <strong>addition</strong> and <strong>scalar multiplication</strong> should hold as those in <span class="math inline">\(\mathbb{R}^{2}\)</span>. Let’s formally call this <span class="math inline">\(n\)</span>-dimensional ordered list a <em>vector</em>. Usually, a <strong>vector</strong> is expressed in a lower-case boldface letter; for displaying the entries or elements of a vector, one can use an vertical array (<em>column vector</em>) surrounded by square (or curved) brackets: <span class="math display">\[\mathbf{a}=\left[\begin{array}{c}
a_{1}\\
\vdots\\
a_{n}
\end{array}\right],\;\mathbf{b}=\left[\begin{array}{c}
b_{1}\\
\vdots\\
b_{n}
\end{array}\right],\quad\mathbf{a}+\mathbf{b}=\left[\begin{array}{c}
a_{1}+b_{1}\\
\vdots\\
a_{n}+b_{n}
\end{array}\right],\; c\mathbf{a}=\left[\begin{array}{c}
c\times a_{1}\\
\vdots\\
c\times a_{n}
\end{array}\right].\]</span></p>
<p>Like the addition of 2D order lists, when we add one <strong>vector</strong> to another <strong>vector</strong>, the <strong>addition</strong> should take into account the order of the <strong>entires</strong> of these vectors. Each <em>entry</em> or component of the vector is called the <em>scalar</em>. when we <strong>scale</strong> the vector by some scalar <span class="math inline">\(c\)</span>, this scalar should multiple all components of the vector. The vector <span class="math inline">\(\mathbf{a}\in \mathbb{R}^n\)</span> is of <em>size</em> <span class="math inline">\(n\)</span>, and it is called the <em><span class="math inline">\(n\)</span>-vector</em>. Two equivalent vectors <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span>, namely <span class="math inline">\(\mathbf{a}=\mathbf{b}\)</span>, implies that they have the same size and the same corresponding entries.</p>
<p>The <em>arithmetic axioms</em> or the <em>algebra axioms</em> of vectors are similar to those rules of real numbers. Suppose that <span class="math inline">\(\mathbf{a}\)</span>, <span class="math inline">\(\mathbf{b}\)</span>, and <span class="math inline">\(\mathbf{c}\)</span> are <span class="math inline">\(n\)</span>-vectors, namely being of the same size, and suppose that <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span> are scalars, the axioms can be summarized as follows:</p>
<ul>
<li>addition <span class="math inline">\(+\)</span> (or called <em>commutative group</em>)<label for="tufte-sn-153" class="margin-toggle sidenote-number">153</label><input type="checkbox" id="tufte-sn-153" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">153</span> The <strong>commutative group</strong> is a concept from abstract algebra. The <strong>commutative group</strong> generalizes the operations that perform similarly as the arithmetic of addition of <a href="sub-continuity.html#sub:rational">integers</a>. Here <span class="math inline">\(+\)</span> operation stands for the operation defining the abstract commutative group rather than the simple addition of integers. That is, if we replace <span class="math inline">\(\mathbf{a}, \mathbf{b}\in\mathbb{R}^n\)</span> with <span class="math inline">\(a, b\in\mathbb{Z}\)</span>, the axioms still hold. Moreover, if we use some other objects instead of vector <span class="math inline">\(\mathbf{a}, \mathbf{b}\)</span>, and if those objects are from a commutative group, they should also satisfy the <strong>axioms</strong> regarding this <span class="math inline">\(+\)</span> operation.</span>:</li>
</ul>
<ol style="list-style-type: decimal">
<li>
<em>commutativity</em> : <span class="math inline">\(\mathbf{a}+\mathbf{b}=\mathbf{b}+\mathbf{a}\)</span>
</li>
<li>
<em>associativity</em> : <span class="math inline">\((\mathbf{a}+\mathbf{b})+\mathbf{c}=\mathbf{a}+(\mathbf{b}+\mathbf{c})\)</span>
</li>
<li>
<em>existence of zero vector</em> (<em>existence of an identity element</em>) : <span class="math inline">\(\mathbf{a}+\mathbf{0}=\mathbf{a}\)</span>
</li>
<li>
<em>existence of negative vector</em> (<em>existence of inverse elements</em>) : <span class="math inline">\(\mathbf{a}+(-\mathbf{a})=\mathbf{0}\)</span>
</li>
</ol>
<ul>
<li>scalar multiplication <span class="math inline">\(\times\)</span> (We often ignore the sign of scalar multiplication.):<br>
</li>
</ul>
<ol style="list-style-type: decimal">
<li>
<em>associativity</em> : <span class="math inline">\(k(l\mathbf{a})=(k\times l)\mathbf{a}\)</span>
</li>
<li>
<em>multiplication of identity elements</em> :<br>
</li>
</ol>
<ol style="list-style-type: lower-roman">
<li>one (identity element for multiplication) <span class="math inline">\(1\mathbf{a}=\mathbf{a}\)</span>,</li>
<li>zero (identity element for addition)<span class="math inline">\(0\mathbf{a}=\mathbf{0}\)</span>,</li>
<li>and zero vector (identity element for <strong>vector addition</strong>): <span class="math inline">\(k\mathbf{0}=\mathbf{0}\)</span>
</li>
</ol>
<ol start="3" style="list-style-type: decimal">
<li>
<em>distributivity</em> for vectors and for scalars: <span class="math inline">\(k(\mathbf{a}+\mathbf{b})=k\mathbf{a}+k\mathbf{b}\)</span>, <span class="math inline">\((k+l)\mathbf{a}=k\mathbf{a}+l\mathbf{a}\)</span>
</li>
</ol>
<p>These axioms (<strong>commutativity</strong>, <strong>associativty</strong>, <strong>negative vector</strong>) in 2D can be easily verified by figure <a href="ch-vecMat.html#fig:Vector">10.2</a>. Note that zero vector <span class="math inline">\(\mathbb{R}^n\)</span> is the origin in the <span class="math inline">\(n\)</span>-dimension. Note that a <em>standard unit vector</em> is a vector with all zero elements except one unit element. For example, <span class="math display">\[\mathbf{e}_{1}=\left[\begin{array}{c}
1\\
0\\
0
\end{array}\right],\:\mathbf{e}_{2}=\left[\begin{array}{c}
0\\
1\\
0
\end{array}\right],\:\mathbf{e}_{3}=\left[\begin{array}{c}
0\\
0\\
1
\end{array}\right]\]</span>
are the three <strong>standard unit vectors</strong> in <span class="math inline">\(\mathbb{R}^{3}\)</span>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:3Dvector"></span>
<img src="fig/Part3/3Dvector.gif" alt="Represent a 3D vector in a linear combination" width="100%"><!--
<p class="caption marginnote">-->Figure 10.3: Represent a 3D vector in a linear combination<!--</p>-->
<!--</div>--></span>
</p>
<p>A <em>linear combination</em> is formed by combining some <strong>additions</strong> and <strong>scalar multiplications</strong> of some <strong>vectors</strong>. Let <span class="math inline">\(\mathbf{x}_{1},\dots,\mathbf{x}_{m}\)</span> be <span class="math inline">\(n\)</span>-vectors, and let <span class="math inline">\(\beta_{1},\dots,\beta_{m}\)</span> be scalars, then the <span class="math inline">\(n\)</span>-vector <span class="math display">\[\beta_{1}\mathbf{x}_{1}+\cdots+\beta_{m}\mathbf{x}_{m}\]</span>
is a <strong>linear combination</strong> of the vectors <span class="math inline">\(\mathbf{x}_{1},\dots,\mathbf{x}_{m}\)</span>. The <strong>scalars</strong> <span class="math inline">\(\beta_{1},\dots,\beta_{m}\)</span> are the coefficients of this <strong>linear combination</strong>. We can write any <span class="math inline">\(n\)</span>-vector <span class="math inline">\(\mathbf{x}\)</span> as a <strong>linear combination</strong> of the <strong>standard unit vectors</strong>,<span class="math display">\[\mathbf{x}=x_{1}\mathbf{e}_{1}+\cdots+x_{n}\mathbf{e}_{n}\]</span>
where <span class="math inline">\(x_{i}\)</span> is the <span class="math inline">\(i\)</span>-th entry of <span class="math inline">\(\mathbf{x}\)</span>, and <span class="math inline">\(\mathbf{e}_{i}\)</span> is the <span class="math inline">\(i\)</span>-th <strong>standard unit vector</strong>. A 3D illustration of the linear combination is given in figure <a href="ch-vecMat.html#fig:3Dvector">10.3</a>.</p>
<p>An essential aspect we didn’t mention is about the multiplication or the <strong>product</strong> of two <strong>vectors</strong>. As any vector stores the relevant information of direction and length, the product should preserve the metric information, i.e., the measurement of the angles and the lengths of the vectors. The length of an <span class="math inline">\(n\)</span>-vector <span class="math inline">\(\mathbf{x}\)</span> is given by the <a href="sub-continuity.html#sub:continuousFunc">Euclidean distance</a> from the origin
<span class="math display">\[\mbox{d}(\mathbf{x},\mathbf{0})=\sqrt{x_{1}^{2}+\cdots+x_{n}^{2}}=\|\mathbf{x}\|\in\mathbb{R}\]</span>
where <span class="math inline">\(\|\cdot\|\)</span> denotes a <em>norm</em>, a function that assigns a strictly positive length to a vector. The <strong>norm</strong> of the vector <span class="math inline">\(\mathbf{x}\)</span> is equivalent to the <a href="sub-continuity.html#sub:continuousFunc"><span class="math inline">\(l_{2}\)</span>-distance</a> function for <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{0}\)</span>, namely <span class="math inline">\(\mbox{d}(\mathbf{x},\mathbf{0})\)</span>. For zero vector, the length is also zero, thus <span class="math inline">\(\|\mathbf{0}\|=0\)</span>.<label for="tufte-sn-154" class="margin-toggle sidenote-number">154</label><input type="checkbox" id="tufte-sn-154" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">154</span> The <strong>norm</strong> is induced by the <a href="sub-continuity.html#sub:continuousFunc">distance</a>, i.e. <span class="math inline">\(\mbox{d}(\mathbf{x},\mathbf{y})=\|\mathbf{x}-\mathbf{y}\|\)</span>. Different <strong>norms</strong> associate with different distance functions, but not all distance functions come from norms (see chapter <a href="ch-optApp.html#sub:appSys">15.1</a>).</span></p>
<p>The product of two vectors is called the <em>inner product</em>, and can be defined as follows:
<span class="math display" id="eq:inner-1">\[\begin{equation}
\langle\mathbf{a},\,\mathbf{b}\rangle=a_{1}b_{1}+\cdots+a_{n}b_{n}=\sum_{i=1}^{n}a_{i}b_{i}.
\tag{10.2} 
\end{equation}
\]</span>
On the other hand, one can also define the <strong>inner product</strong> by
<span class="math display" id="eq:inner-2">\[\begin{equation}
\langle\mathbf{a},\,\mathbf{b}\rangle=
\begin{cases}
\|\mathbf{a}\|\|\mathbf{b}\|\cos\theta, &amp; \mbox{ if }\mathbf{a},\mathbf{b}\neq0,\\
0, &amp; \mbox{ if }\mathbf{a}=0,\mbox{ or }\mathbf{b}=0,
\end{cases}
\tag{10.3}
\end{equation}
\]</span>
where <span class="math inline">\(\theta\)</span> is the smallest angle between <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span>. These two definitions are equivalent.</p>
<div class="solution">
<p class="solution-begin">
Proof
</p>
<div class="solution-body">
<p>Expression <a href="ch-vecMat.html#eq:inner-1">(10.2)</a> to expression <a href="ch-vecMat.html#eq:inner-2">(10.3)</a></p>
<p>If either <span class="math inline">\(\mathbf{a}=0\)</span> or <span class="math inline">\(\mathbf{b}=0\)</span>, then <span class="math inline">\(\sum_{i=1}^{n}a_{i}b_{i}=0\)</span> and <span class="math inline">\(\|\mathbf{a}\|\|\mathbf{b}\|=0\)</span>. Thus two expressions give the same answer.</p>
<p>For a non-zero inner product, let’s first consider the case in <span class="math inline">\(\mathbb{R}^2\)</span>. Any vector satisfying <span class="math inline">\(\|\mathbf{x}\|=1\)</span> is a <em>unit vector</em> (not necessarily being a <strong>standard unit vector</strong>). Note that the vector <span class="math inline">\(\mathbf{u}\)</span>
satisfying <span class="math display">\[\mathbf{u}=\left[\begin{array}{c}
\cos\theta\\
\sin\theta
\end{array}\right],\:\|\mathbf{u}\|=\sqrt{\cos^{2}\theta+\sin^{2}\theta}=1\]</span>
is a <strong>unit vector</strong>. The unit vector <span class="math inline">\(\mathbf{u}\)</span> and another unit vector <span class="math inline">\(\mathbf{u}'\)</span>
have the inner product of their angle differences <span class="math display">\[\langle\mathbf{u},\,\mathbf{u}'\rangle=\cos\theta\times\cos\theta'+\sin\theta\times\sin\theta'=\cos(\theta-\theta')\]</span>
by the trigonometry formula.<label for="tufte-sn-155" class="margin-toggle sidenote-number">155</label><input type="checkbox" id="tufte-sn-155" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">155</span> When <span class="math inline">\(\theta'=0\)</span>, the unit vector <span class="math inline">\(\mathbf{u}'\)</span> is the <strong>standard unit vector</strong> <span class="math inline">\(\mathbf{e}_{1}\)</span>. Then <span class="math inline">\(\mathbf{u}\)</span> and the <strong>standard unit vector</strong> <span class="math inline">\(\mathbf{e}_{1}\)</span> has the <strong>inner product</strong> <span class="math display">\[\langle\mathbf{u},\,\mathbf{e}_{1}\rangle=\cos\theta\times1+\sin\theta\times0=\cos\theta.\]</span></span>
Thus we can conclude that for any two unit vectors in <span class="math inline">\(\mathbb{R}^2\)</span>, the two definitions of the inner product are equivalent.</p>
<p>Now consider the general case in <span class="math inline">\(\mathbb{R}^n\)</span>. It is always true that <span class="math inline">\(\mathbf{a}/\|\mathbf{a}\|\)</span> and <span class="math inline">\(\mathbf{b}/\|\mathbf{b}\|\)</span>
are <strong>unit vectors</strong>.<label for="tufte-sn-156" class="margin-toggle sidenote-number">156</label><input type="checkbox" id="tufte-sn-156" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">156</span> Take <span class="math inline">\(\mathbf{a}/\|\mathbf{a}\|\)</span> as an example.
<span class="math display">\[\begin{align*}
  \left\Vert \frac{\mathbf{a}}{\|\mathbf{a}\|}\right\Vert   =&amp;\sum_{i=1}^{n}\frac{1}{\|\mathbf{a}\|}\left(a_{1}^{2}+\cdots+a_{n}^{2}\right)\\
    =&amp;\frac{1}{\|\mathbf{a}\|}\sum_{i=1}^{n}\left(a_{1}^{2}+\cdots+a_{n}^{2}\right)
    \\=
    &amp;\frac{1}{\|\mathbf{a}\|}\|\mathbf{a}\|=1.
    \end{align*}\]</span>
Transforming <span class="math inline">\(\mathbf{a}\)</span> into <span class="math inline">\(\mathbf{a}/\|\mathbf{a}\|\)</span> is called the <em>normalization of the vector</em> <span class="math inline">\(\mathbf{a}\)</span>.</span> Therefore, trigonometry formula tells us <span class="math display">\[\left\langle \frac{\mathbf{a}}{\|\mathbf{a}\|},\,\frac{\mathbf{b}}{\|\mathbf{b}\|}\right\rangle =\cos\theta\]</span>
which implies <span class="math inline">\(\langle\mathbf{a},\,\mathbf{b}\rangle=\|\mathbf{a}\|\|\mathbf{b}\|\cos\theta.\)</span> The result follows.<label for="tufte-sn-157" class="margin-toggle sidenote-number">157</label><input type="checkbox" id="tufte-sn-157" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">157</span> Note that <span class="math display">\[\begin{align*}\left\langle \frac{\mathbf{a}}{\|\mathbf{a}\|},\,\frac{\mathbf{b}}{\|\mathbf{b}\|}\right\rangle =&amp;\frac{1}{\|\mathbf{a}\|\|\mathbf{b}\|}\sum_{i=1}^{n}a_{i}b_{i}\\=&amp;\frac{\langle\mathbf{a},\,\mathbf{b}\rangle}{\|\mathbf{a}\|\|\mathbf{b}\|}.\end{align*}\]</span></span></p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>There is also an alternative way of expressing an <strong>inner product</strong> as a product of a <strong>row vector</strong> and a <strong>column vector</strong>. See chapter <a href="ch-vecMat.html#sub:matrix">10.3</a>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:projection"></span>
<img src="fig/Part3/projection.png" alt="Projection" width="100%"><!--
<p class="caption marginnote">-->Figure 10.4: Projection<!--</p>-->
<!--</div>--></span>
</p>
<p>The expression <a href="ch-vecMat.html#eq:inner-2">(10.3)</a> can imply several important results. If <span class="math inline">\(\langle\mathbf{a},\,\mathbf{b}\rangle=0\)</span> in equation <a href="ch-vecMat.html#eq:inner-1">(10.2)</a> and <span class="math inline">\(\mathbf{a},\mathbf{b}\neq0\)</span>, then the expression <a href="ch-vecMat.html#eq:inner-2">(10.3)</a> says that <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span> must be perpendicular (orthogonal to each other), namely <span class="math inline">\(\theta=90^{\circ}\)</span>. Also, since <span class="math inline">\(|\cos\theta|\)</span> never exceeds one, expression <a href="ch-vecMat.html#eq:inner-2">(10.3)</a> gives the following inequality <span class="math display">\[\left|\frac{\langle\mathbf{a},\,\mathbf{b}\rangle}{\|\mathbf{a}\|\|\mathbf{b}\|}\right|\leq1,\;\mbox{ or say}\left|\langle\mathbf{a},\,\mathbf{b}\rangle\right|\leq\|\mathbf{a}\|\|\mathbf{b}\|,\]</span>
which is called <em>Schwarz inequality</em>. Because the norm <span class="math inline">\(\|\cdot\|\)</span> is a <a href="sub-continuity.html#sub:continuousFunc">distance function</a>, it should also satisfy the <a href="sub-continuity.html#sub:continuousFunc">triangular inequality</a> <span class="math display">\[\|\mathbf{a}+\mathbf{b}\|\leq\|\mathbf{a}\|+\|\mathbf{b}\|.\]</span>
Finally, by the expression <a href="ch-vecMat.html#eq:inner-2">(10.3)</a>, we can deduce a useful formula called the <em>orthogonal projection</em> formula. Consider a situation in which one vector <span class="math inline">\(\mathbf{a}\)</span> shall be <strong>projected orthogonally</strong> onto another vector <span class="math inline">\(\mathbf{b}\)</span>
in order to create a new vector <span class="math inline">\(\mathbf{c}\)</span>. Since <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{c}\)</span>
make up a triangle with a right angle (see figure <a href="ch-vecMat.html#fig:projection">10.4</a>), by the definition of cosine function, we have <span class="math inline">\(\cos\theta=\|\mathbf{c}\|/\|\mathbf{a}\|\)</span> or <span class="math inline">\(\|\mathbf{c}\|=\cos\theta\|\mathbf{a}\|\)</span>. Then by <strong>normalizing</strong> <span class="math inline">\(\mathbf{b}\)</span> and <span class="math inline">\(\mathbf{c}\)</span>, we have <span class="math inline">\(\mathbf{c}/\|\mathbf{c}\|=\mathbf{b}/\|\mathbf{b}\|\)</span>, and hence <span class="math display">\[\mathbf{c}=\|\mathbf{c}\|\frac{\mathbf{b}}{\|\mathbf{b}\|}=\|\mathbf{a}\|\cos\theta\frac{\mathbf{b}}{\|\mathbf{b}\|}.\]</span>
By the definition of inner product (expression <a href="ch-vecMat.html#eq:inner-2">(10.3)</a>), we have
<span class="math display" id="eq:proj">\[
\begin{equation}
\mathbf{c}=\|\mathbf{a}\|\|\mathbf{b}\|\cos\theta\frac{\mathbf{b}}{\|\mathbf{b}\|^{2}}=\frac{\langle\mathbf{a},\,\mathbf{b}\rangle}{\|\mathbf{b}\|^{2}}\mathbf{b}.
\tag{10.4}
\end{equation}
\]</span>
The term <span class="math inline">\(\langle\mathbf{a},\,\mathbf{b}\rangle/\|\mathbf{b}\|^{2}\)</span> gives the <em>orthogonal projection</em> of <span class="math inline">\(\mathbf{a}\)</span> onto <span class="math inline">\(\mathbf{b}\)</span>.<label for="tufte-sn-158" class="margin-toggle sidenote-number">158</label><input type="checkbox" id="tufte-sn-158" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">158</span> Note that <span class="math inline">\(\|\mathbf{b}\|^{2}=\langle\mathbf{b},\,\mathbf{b}\rangle\)</span>, one can also write the ** orthogonal projection** of <span class="math inline">\(\mathbf{a}\)</span> onto <span class="math inline">\(\mathbf{b}\)</span> as <span class="math display">\[\frac{\langle\mathbf{a},\,\mathbf{b}\rangle}{\langle\mathbf{b},\,\mathbf{b}\rangle}\mathbf{b}\]</span>.</span></p>
<p>With the <strong>projection formula</strong> <a href="ch-vecMat.html#eq:proj">(10.4)</a>, we can deduce the following rules (axioms).</p>
<ul>
<li>Rules of <strong>inner product</strong> <span class="math inline">\(\langle\cdot,\,\cdot\rangle\)</span> for <span class="math inline">\(\mathbf{a},\mathbf{b}\in\mathbb{R}^{n}\)</span> and <span class="math inline">\(k\in\mathbb{R}\)</span>:</li>
</ul>
<ol style="list-style-type: decimal">
<li>
<em>Commutativity</em> : <span class="math inline">\(\langle\mathbf{a},\,\mathbf{b}\rangle=\langle\mathbf{b},\,\mathbf{a}\rangle\)</span>
</li>
<li>
<em>Associativity</em> : <span class="math inline">\(k\langle\mathbf{a},\,\mathbf{b}\rangle=\langle k\mathbf{a},\,\mathbf{b}\rangle=\langle\mathbf{a},\, k\mathbf{b}\rangle\)</span>
</li>
<li>
<em>Distributivity</em> : <span class="math inline">\(\langle\mathbf{a},\,\mathbf{b}+\mathbf{c}\rangle=\langle\mathbf{a},\,\mathbf{b}\rangle+\langle\mathbf{a},\,\mathbf{c}\rangle\)</span>
</li>
</ol>
<div class="solution">
<p class="solution-begin">
Proof
</p>
<div class="solution-body">
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:distributivity"></span>
<img src="fig/Part3/distributivity.gif" alt="Distributivity" width="100%"><!--
<p class="caption marginnote">-->Figure 10.5: Distributivity<!--</p>-->
<!--</div>--></span>
</p>
<p><strong>Commutativity</strong> and <strong>associativity</strong> come directly from the definition <a href="ch-vecMat.html#eq:inner-1">(10.2)</a> and <a href="ch-vecMat.html#eq:inner-2">(10.3)</a>. To see <strong>distributivity</strong>, we need to see that the sum of the <strong>projections</strong> is equal to the <strong>projection</strong> of the sum, (figure <a href="ch-vecMat.html#fig:distributivity">10.5</a>). It means<span class="math display">\[\frac{\langle\mathbf{b}+\mathbf{c},\,\mathbf{a}\rangle}{\|\mathbf{a}\|^{2}}\mathbf{a}=\frac{\langle\mathbf{b},\,\mathbf{a}\rangle}{\|\mathbf{a}\|^{2}}\mathbf{a}+\frac{\langle\mathbf{c},\,\mathbf{a}\rangle}{\|\mathbf{a}\|^{2}}\mathbf{a}.\]</span>
We only need to focus on the coefficients of this equality. By canceling out <span class="math inline">\(\|\mathbf{a}\|^{2}\)</span>
on the both sides, we have <span class="math display">\[\langle\mathbf{b}+\mathbf{c},\,\mathbf{a}\rangle=\langle\mathbf{b},\,\mathbf{a}\rangle+\langle\mathbf{c},\,\mathbf{a}\rangle.\]</span>
Then interchanging the positions of <span class="math inline">\(\mathbf{a}\)</span> by the <strong>commutativity</strong> rule gives the desired result.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>As the expression <a href="ch-vecMat.html#eq:inner-1">(10.2)</a> defines a <a href="sub-continuity.html#sub:continuousFunc">Euclidean distance</a> for two <span class="math inline">\(n\)</span>-vectors <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span>, the <strong>inner product</strong> is a particular type of <a href="sub-continuity.html#sub:continuousFunc">distance</a>. Note that the <a href="sub-continuity.html#sub:continuousFunc">five axioms of distance functions</a> do not include everything that can be said about distance in our common sense of geometry. Pythagoras’ theorem, for instance, cannot be deduced from those five axioms but it can be deduced by the rules of <strong>inner product</strong>. If <span class="math inline">\(\mathbf{a}\)</span>
and <span class="math inline">\(\mathbf{b}\)</span>
are <strong>orthogonal</strong> to each other, then <span class="math display">\[\|\mathbf{a}+\mathbf{b}\|^{2}=\|\mathbf{a}\|^{2}+\|\mathbf{b}\|^{2}\]</span>
gives a <em>generalized Pythagoras’ theorem</em>.<label for="tufte-sn-159" class="margin-toggle sidenote-number">159</label><input type="checkbox" id="tufte-sn-159" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">159</span> 
<span class="math display">\[
\begin{align*}
\|\mathbf{a}+\mathbf{b}\|^{2}&amp;= \langle\mathbf{a}+\mathbf{b},\,\mathbf{a}+\mathbf{b}\rangle\\
&amp;\overset{(a)}{=}   \langle\mathbf{a},\,\mathbf{a}+\mathbf{b}\rangle+\langle\mathbf{b},\,\mathbf{a}+\mathbf{b}\rangle\\
&amp;\overset{(b)}{=}   \langle\mathbf{a},\,\mathbf{a}\rangle+\langle\mathbf{a},\,\mathbf{b}\rangle+\langle\mathbf{b},\,\mathbf{a}\rangle+\langle\mathbf{b},\,\mathbf{b}\rangle\\
&amp;=  \|\mathbf{a}\|^{2}+\|\mathbf{b}\|^{2}+2\langle\mathbf{a},\,\mathbf{b}\rangle\\
&amp;\overset{(c)}{=}\|\mathbf{a}\|^{2}+\|\mathbf{b}\|^{2}
\end{align*}
\]</span>
where <span class="math inline">\(\overset{(a)}{=}\)</span> and <span class="math inline">\(\overset{(b)}{=}\)</span> use the <strong>distributive rule</strong>, <span class="math inline">\(\overset{(c)}{=}\)</span> use the <strong>orthogonality</strong> <span class="math inline">\(\langle\mathbf{a},\,\mathbf{b}\rangle=0\)</span>.</span> In this sense, the definition of <strong>inner product</strong> (expression <a href="ch-vecMat.html#eq:inner-2">(10.3)</a>) actually specifies the <strong>projective</strong> geometry properties of vectors.</p>
</div>
<div id="sub:linearity" class="section level2">
<h2>
<span class="header-section-number">10.2</span> Example: Linearity</h2>
<p><a href="#sub:Vector">Vectors</a> establish the basic objects in the numerical computation. Consider a function <span class="math inline">\(f:\mathbb{R}^{n}\mapsto\mathbb{R}\)</span>. We can model this function by saying that it maps from real <span class="math inline">\(n\)</span>-vectors to real numbers such as <span class="math inline">\(f(\mathbf{x})=f(x_{1},\dots,x_{n})\)</span>. And the <a href="#sub:Vector">inner product</a> is such a function:<span class="math display">\[f(\mathbf{x})=\langle\mathbf{b},\,\mathbf{x}\rangle=b_{1}x_{1}+\cdots+b_{n}x_{n}\]</span>
where <span class="math inline">\(b_{1},\dots,b_{n}\)</span> are the coefficients. The <a href="#sub:Vector">inner product</a> can uniquely represent a class of functions called <strong>linear functions</strong>. The linear function is one of the most fundamental functions. For example, in economics, the total income or the total expenses can be expressed by an inner product of quantities and prices of <span class="math inline">\(n\)</span> items.</p>
<p>A function <span class="math inline">\(f\)</span> is <em>linear</em> or <em>superposition</em> if <span class="math display">\[f(\alpha x+\beta y)=\alpha f(x)+\beta f(y)\]</span>
for <a href="#sub:Vector">scalars</a> <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. The property <span class="math inline">\(f(\alpha x)=\alpha f(x)\)</span> is called the <em>homogeneity</em>, and the property <span class="math inline">\(f(x+y)=f(x)+f(y)\)</span> is called the <em>additivity</em>.<label for="tufte-sn-160" class="margin-toggle sidenote-number">160</label><input type="checkbox" id="tufte-sn-160" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">160</span> We can easily find that if the linear function is an inner product, then the <strong>aditivity</strong> and <strong>homogeneity</strong> are respectively corresponding to the rules of <a href="#sub:Vector">addition</a> and <a href="#sub:Vector">scalar multiplication</a> of vectors.</span> Combining <strong>homogenity</strong> and <strong>additivity</strong> gives the <strong>superposition</strong>. For any function <span class="math inline">\(f:\mathbb{R}^{n}\mapsto\mathbb{R}\)</span>, if <span class="math inline">\(f\)</span> is <strong>linear</strong>, then <span class="math inline">\(f\)</span> can be uniquely represented by an <a href="#sub:">inner product</a> of its argument <span class="math inline">\(\mathbf{x}\in\mathbb{R}^{n}\)</span> with some fixed vector <span class="math inline">\(\mathbf{b}\in\mathbb{R}^{n}\)</span>, namely <span class="math inline">\(f(\mathbf{x})=\langle\mathbf{b},\,\mathbf{x}\rangle\)</span>.</p>
<div class="solution">
<p class="solution-begin">
Proof <span id="sol-start-34" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-34', 'sol-start-34')"></span>
</p>
<div id="sol-body-34" class="solution-body" style="display: none;">
<p>An arbitrary <span class="math inline">\(n\)</span>-vector <span class="math inline">\(\mathbf{x}\)</span>
has the <a href="#sub:Vector">linear combination</a> <span class="math inline">\(\mathbf{x}=x_{1}\mathbf{e}_{1}+\cdots+x_{n}\mathbf{e}_{n}\)</span>. For a <strong>linear function</strong> <span class="math inline">\(f\)</span>, the <strong>superposition</strong> gives<span class="math display">\[f(\mathbf{x})=f(x_{1}\mathbf{e}_{1}+\cdots+x_{n}\mathbf{e}_{n})=x_{1}f(\mathbf{e}_{1})+\cdots+x_{n}f(\mathbf{e}_{n})=\langle\mathbf{x},\,\mathbf{b}\rangle\]</span>
where <span class="math inline">\(\mathbf{b}\)</span> is the vector of <span class="math inline">\(f(\mathbf{e}_{1}),\dots,f(\mathbf{e}_{n})\)</span>.
By <a href="#sub:Vector">commutativity</a> of the inner product, we have <span class="math inline">\(f(\mathbf{x})=\langle\mathbf{b},\,\mathbf{x}\rangle\)</span>. To see this representation is unique. Suppose there is another vector <span class="math inline">\(\mathbf{c}\)</span> such that <span class="math inline">\(f(\mathbf{x})=\langle\mathbf{c},\,\mathbf{x}\rangle\)</span> for all <span class="math inline">\(\mathbf{x}\in\mathbb{R}^{n}\)</span>. Then let <span class="math inline">\(\mathbf{x}\)</span> be any <a href="#sub:Vector">standard unit vector</a> <span class="math inline">\(\mathbf{e}_{i}\)</span>, we have <span class="math inline">\(f(\mathbf{e}_{i})=\langle\mathbf{c},\,\mathbf{e}_{i}\rangle=c_{i}\)</span>. Similarly, if we use the other representation, then <span class="math inline">\(f(\mathbf{e}_{i})=\langle\mathbf{b},\,\mathbf{e}_{i}\rangle=b_{i}\)</span>. It means <span class="math inline">\(b_{i}=c_{i}\)</span> for any <span class="math inline">\(i=1,\dots,n\)</span>. Thus <span class="math inline">\(\mathbf{b}=\mathbf{c}\)</span>, the representation is unique.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p><span class="newthought">Multi-valued function</span></p>
<p>The <a href="#sub:Vector">vector</a> also allows us to study some function mapping to higher dimensions <span class="math inline">\(\mathbf{f}:\mathbb{R}\mapsto\mathbb{R}^{n}\)</span>. For example, the dynamical 3D spiral in figure <a href="sub-inferknow.html#fig:3DSpiral">4.6</a> is such a function <span class="math display">\[\mathbf{f}(t)=\left[\begin{array}{c}
f_{1}(t)\\
f_{2}(t)\\
f_{3}(t)
\end{array}\right]=\left[\begin{array}{c}
\mbox{e}^{t}\cos t,\\
\mbox{e}^{t}\sin t\\
t
\end{array}\right].\]</span>
The <a href="sub-continuity.html#sub:continuity">continuity</a> for <span class="math inline">\(\mathbf{f}:\mathbb{R}\mapsto\mathbb{R}^{n}\)</span> should take into account the <a href="sub-continuity.html#sub:continuity">continuity</a> in each dimension, namely the <a href="sub-continuity.html#sub:continuity">continuity</a> of every <span class="math inline">\(f_{i}(x)\)</span> in the vector <span class="math inline">\(\mathbf{f}(x)\)</span>. The <a href="sub-incomplete.html#sub:infinity">limit</a> <span class="math inline">\(\lim_{x\rightarrow a}\mathbf{f}(x)\)</span>
does not exists if and only if there is a sequence <span class="math inline">\(x_{n}\rightarrow a\)</span> such that <span class="math inline">\(\mathbf{f}(x_{n})\)</span> does not <a href="sub-incomplete.html#sub:infinity">converge</a>. The function <span class="math inline">\(\mathbf{f}(x)\)</span> is <a href="sub-continuity.html#sub:continuity">continuous</a> at a point <span class="math inline">\(a\)</span> if <span class="math inline">\(\lim_{x\rightarrow a}\mathbf{f}(x)=\mathbf{f}(a)\)</span>.</p>
<p><span class="newthought">Gradient </span></p>
<p>With the properties of function values in <span class="math inline">\(\mathbb{R}^n\)</span>, let’s consider a special <a href="sub-set-theory.html#sub:func">mapping</a> from <span class="math inline">\(\mathbb{R}^{n}\)</span>
to <span class="math inline">\(\mathbb{R}^{n}\)</span> which is called the <strong>gradient</strong>. Consider a <a href="sub-calculus.html#sub:diffInt">differentiable</a> function <span class="math inline">\(f:\mathbb{R}^{n}\mapsto\mathbb{R}\)</span>, the <em>gradient</em> of <span class="math inline">\(f\)</span> is an <span class="math inline">\(n\)</span>-vector <span class="math display">\[\nabla f(\mathbf{z})=\left[\begin{array}{c}
\left.\frac{\partial f}{\partial x_{1}}(\mathbf{x})\right|_{\mathbf{x}=\mathbf{z}}\\
\vdots\\
\left.\frac{\partial f}{\partial x_{n}}(\mathbf{x})\right|_{\mathbf{x}=\mathbf{z}}
\end{array}\right]\]</span>
where <span class="math inline">\(\partial f/\partial x_{i}\)</span> is the <a href="ch-DE.html#sub:pde">partial derivative</a> <span class="math display">\[\frac{\partial f}{\partial x_{i}}(\mathbf{x})=\lim_{\epsilon\rightarrow0}\frac{f(x_{1},\dots,x_{i}+\epsilon,\dots x_{n})-f(\mathbf{x})}{\epsilon}.\]</span>
We can <a href="sub-calculus.html#sub:Taylor">linearize</a> a nonlinear (vector) function <span class="math inline">\(f:\mathbb{R}^{n}\mapsto\mathbb{R}\)</span>
by <a href="sub-calculus.html#sub:Taylor">Taylor series</a> such that <span class="math display">\[f(\mathbf{x})\approx f(\mathbf{z})+\left\langle \nabla f(\mathbf{z}),\,(\mathbf{x}-\mathbf{z})\right\rangle.\]</span>
The first term in the <a href="sub-calculus.html#sub:Taylor">Taylor series</a> is a constant vector <span class="math inline">\(f(\mathbf{z})\)</span>, the second term is the inner product of the <strong>gradient</strong> of <span class="math inline">\(f\)</span> at <span class="math inline">\(\mathbf{z}\)</span> and the difference between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{z}\)</span>. This first order Taylor expansion is a <strong>linear function</strong> <span class="math inline">\(\left\langle \nabla f(\mathbf{z}),\,(\mathbf{x}-\mathbf{z})\right\rangle\)</span> plus a constant vector <span class="math inline">\(f(\mathbf{z})\)</span>.<label for="tufte-sn-161" class="margin-toggle sidenote-number">161</label><input type="checkbox" id="tufte-sn-161" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">161</span> A linear function <span class="math inline">\(\langle\mathbf{b},\,\mathbf{x}\rangle\)</span> plus a constant vector <span class="math inline">\(\mathbf{a}\)</span> is called an <em>affine function</em> <span class="math inline">\(f(\mathbf{x})=\langle\mathbf{b},\,\mathbf{x}\rangle+\mathbf{a}\)</span>. In many applied contexts <strong>affine functions</strong> are also called <strong>linear functions</strong>. However, in order to satisfy the <strong>superposition</strong> property, the argument <span class="math inline">\(\alpha x+\beta y\)</span> should satisfy <span class="math inline">\(\alpha+\beta=1\)</span>. To see this, note that
<span class="math display">\[\begin{align*} 
f(\alpha\mathbf{x}+\beta\mathbf{y}) &amp;=  \langle\mathbf{b},\,\alpha\mathbf{x}+\beta\mathbf{y}\rangle+\mathbf{a}\\
&amp;=  \alpha\langle\mathbf{b},\,\mathbf{x}\rangle+\beta\langle\mathbf{b},\,\mathbf{y}\rangle+\mathbf{a}. \end{align*}\]</span>
Using the property <span class="math inline">\(\alpha+\beta=1\)</span>, the previous expression becomes
<span class="math display">\[\begin{align*} 
\alpha\langle\mathbf{b},\,\mathbf{x}\rangle+\beta\langle\mathbf{b},\,\mathbf{y}\rangle+&amp;(\alpha+\beta)\mathbf{a}    =\\
\alpha(\langle\mathbf{b},\,\mathbf{x}\rangle+\mathbf{a})+&amp;\beta(\langle\mathbf{b},\,\mathbf{y}\rangle+\mathbf{a})\\
=   \alpha f(\mathbf{x})+\beta f(\mathbf{y}).
\end{align*}\]</span></span></p>
<p><span class="newthought">Fixed point of vectors </span></p>
<p>Let’s look at a vector version <a href="sub-continuity.html#sub:Cauchy">fixed point</a> result. The previous system <a href="ch-vecMat.html#eq:sem-1">(10.1)</a> can be rewritten as
<span class="math display">\[\begin{align*}
\mathbf{x}=&amp;\left[\begin{array}{c}
x_{1}\\
x_{2}
\end{array}\right]  =\left[\begin{array}{c}
0.4x_{1}-0.3x_{2}+1\\
0.3x_{1}+0.4x_{2}
\end{array}\right]\\
&amp;=\left[\begin{array}{c}
1\\
0
\end{array}\right]  +\left[\begin{array}{c}
g_{1}(\mathbf{x})\\
g_{2}(\mathbf{x})
\end{array}\right]=\mathbf{d}+\mathbf{g}(\mathbf{x})=\mathbf{f}(\mathbf{x})
\end{align*}\]</span>
where <span class="math inline">\(\mathbf{g}:\mathbb{R}^{2}\mapsto\mathbb{R}^{2}\)</span> is a <strong>linear function</strong> (both <span class="math inline">\(g_{1}\)</span> and <span class="math inline">\(g_{2}\)</span> are <strong>linear</strong>), and <span class="math inline">\(\mathbf{f}:\mathbb{R}^{2}\mapsto\mathbb{R}^{2}\)</span> is an <strong>affine function</strong>. The following simple code shows that the system reach the point around <span class="math inline">\(x_{1}=4/3\)</span> and <span class="math inline">\(x_{2}=2/3\)</span>, namely the solution vector of the linear system.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1">x1 =<span class="st"> </span><span class="dv">0</span>; x2 =<span class="st"> </span><span class="dv">0</span>;</a>
<a class="sourceLine" id="cb24-2" data-line-number="2"><span class="cf">for</span> (iter <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>){ </a>
<a class="sourceLine" id="cb24-3" data-line-number="3">  x1 =<span class="st"> </span><span class="fl">0.4</span><span class="op">*</span>x1 <span class="op">-</span><span class="st"> </span><span class="fl">0.3</span><span class="op">*</span>x2 <span class="op">+</span><span class="st"> </span><span class="dv">1</span> ; </a>
<a class="sourceLine" id="cb24-4" data-line-number="4">  x2 =<span class="st"> </span><span class="fl">0.3</span><span class="op">*</span>x1 <span class="op">+</span><span class="st"> </span><span class="fl">0.4</span><span class="op">*</span>x2; </a>
<a class="sourceLine" id="cb24-5" data-line-number="5">  <span class="kw">cat</span>(<span class="st">"At iteration"</span>, iter, <span class="st">"x1 is:"</span>, x1, <span class="st">"x2 is:"</span>, x2, <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</a>
<a class="sourceLine" id="cb24-6" data-line-number="6">}</a></code></pre></div>
<pre><code>## At iteration 1 x1 is: 1 x2 is: 0.3 
## At iteration 2 x1 is: 1.31 x2 is: 0.513 
## At iteration 3 x1 is: 1.3701 x2 is: 0.61623 
## At iteration 4 x1 is: 1.363171 x2 is: 0.6554433 
## At iteration 5 x1 is: 1.348635 x2 is: 0.6667679 
## At iteration 6 x1 is: 1.339424 x2 is: 0.6685343 
## At iteration 7 x1 is: 1.335209 x2 is: 0.6679765 
## At iteration 8 x1 is: 1.333691 x2 is: 0.6672978 
## At iteration 9 x1 is: 1.333287 x2 is: 0.6669052 
## At iteration 10 x1 is: 1.333243 x2 is: 0.666735</code></pre>
<p>From the result table, we can see that the <a href="sub-continuity.html#sub:Cauchy">Lipschitz continuity</a> is satisfied for this <strong>affine</strong> function. <span class="math display">\[\|\mathbf{f}(\mathbf{x})-\mathbf{f}(\mathbf{x}')\|\leq\|\mathbf{x}-\mathbf{x}'\|.\]</span>
The sequence is a <a href="sub-continuity.html#sub:Cauchy">Cauchy sequence</a>.</p>
<p>For a general case, let the fixed point of the system of two equations <span class="math inline">\(x_1=f_{1}(x_1,x_2),\; x_2=f_{2}(x_1,x_2)\)</span>
be <span class="math inline">\((x_1^{*},x_2^{*})\)</span> such that <span class="math inline">\(x_1^{*}=f_{1}(x_1^{*},x_2^{*})\)</span> and <span class="math inline">\(x_2^{*}=f_{2}(x_1^{*},x_2^{*})\)</span>. The fixed-point iteration is <span class="math display">\[x_1^{(k+1)}=f_{1}(x_1^{(k)},x_2^{(k)}),\; x_2^{(k+1)}=f_{2}(x_1^{(k)},x_2^{(k)}).\]</span>
Their paritial derivatives are <a href="sub-continuity.html#sub:Cauchy">continuous</a> on a region that contains the fixed point <span class="math inline">\((x_1^{*},x_2^{*})\)</span>. If <span class="math inline">\((x_1,x_2)\)</span> is sufficiently close to <span class="math inline">\((x_1^{*},x_2^{*})\)</span> and if
<span class="math display">\[\left|\frac{\partial f_{i}}{\partial x_1}(x_1^{*},x_2^{*})\right|+\left|\frac{\partial f_{i}}{\partial x_2}(x_1^{*},x_2^{*})\right|&lt;1\]</span>
for <span class="math inline">\(i=1,2\)</span>, then the iteration converges <a href="ch-DE.html#sub:stab">stably</a> to <span class="math inline">\((x_1^{*},x_2^{*})\)</span>.<label for="tufte-sn-162" class="margin-toggle sidenote-number">162</label><input type="checkbox" id="tufte-sn-162" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">162</span> Otherwise, the iteration might diverge. This will usually be the case if the sum of the magnitudes of the partial derivatives is much larger than <span class="math inline">\(1\)</span>. See chapter <a href="ch-DE.html#sub:stab">8.4</a> for the discussion about the stability of fixed point algorithms.</span></p>
<p><span class="newthought">Linear regression </span></p>
<p>Very often, besides the <a href="sub-set-theory.html#sub:func">input</a> and the <a href="sub-set-theory.html#sub:func">output</a>, the system is also characterized by the parameters. To be precise, we recall the notion of the <a href="ch-CalUn.html#sub:conProb">regression</a> <span class="math inline">\(g(X)=\mathbb{E}[Y|X]\)</span>, a conditional expectation with the output <span class="math inline">\(Y\)</span> and the (conditional) input <span class="math inline">\(X\)</span>. If we let <span class="math inline">\(Y\)</span> be a linear function with an additive <a href="ch-CalUn.html#sub:ex">i.i.d. random variable</a> <span class="math inline">\(\varepsilon\)</span> and a <a href="ch-CalUn.html#sub:divRV">parameter</a> <span class="math inline">\(\beta\)</span> as the coefficient of this linear function such that <span class="math display">\[Y=\beta X+\varepsilon,\]</span> then the regression is a <em>linear regression</em>, or say <span class="math inline">\(g(X)=\beta X\)</span> as <span class="math inline">\(\mathbb{E}[\varepsilon|X]=0\)</span> for the <a href="ch-CalUn.html#sub:ex">i.i.d. random variable</a>.</p>
<p>When the parameter <span class="math inline">\(\beta\)</span> is unknown, we need to <strong>estimate</strong> the value of <span class="math inline">\(\beta\)</span> through the <a href="sub-axioms.html#sub:rec">sequences</a> of observations of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. That is, we want to determine the model parameters that produce the observable data we have recorded. This problem is known as the <em>estimation problem</em>: one looks for the model parameter that presumably generates the data. In data analysis, the observations (realizations) of a random variable <span class="math inline">\(X\)</span> (or <span class="math inline">\(Y\)</span>) are stored by an <em>array</em>, namely the data vector <span class="math inline">\(\mathbf{x}\)</span> (or <span class="math inline">\(\mathbf{y}\)</span>).</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb26-2" data-line-number="2">x=<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>; y=<span class="dv">5</span><span class="op">*</span>x<span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">rnorm</span>(<span class="dv">10</span>); dat=<span class="kw">data.frame</span>(x,y) </a></code></pre></div>
<div id="htmlwidget-7aed86f88d54bd4817b7" style="width:55%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-7aed86f88d54bd4817b7">{"x":{"filter":"none","caption":"<caption>Simulated x and y<\/caption>","autoHideNavigation":false,"data":[["1","2","3","4","5","6","7","8","9","10"],[1,2,3,4,5,6,7,8,9,10],[5.75394424987287,10.6030967478713,12.8039536586928,17.7391881927924,19.4069313602565,31.4411469968232,36.878242046018,39.5412445065851,48.5182626939269,50.2347335736057]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>x<\/th>\n      <th>y<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[1,2]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script><p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:leastSquare"></span>
<img src="fig/Part3/leastSquare.gif" alt="Least square estimate" width="100%"><!--
<p class="caption marginnote">-->Figure 10.6: Least square estimate<!--</p>-->
<!--</div>--></span>
</p>
<p>The table displays ten observations for <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. These are <span class="math inline">\(10\)</span>-vector <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(10\)</span>-vector <span class="math inline">\(\mathbf{y}\)</span>. These vectors are generated by a <a href="ch-CalUn.html#sub:divRV">Monte Carlo simulation</a>.
The parameter recovery simulations have frequently been used to assess the accuracy of the estimation.
We assume that a model accurately captures the processes that generate data, and fit the model to the data so as to draw conclusions from its parameter estimates.</p>
<p>A very commonly used <strong>estimation method</strong> for <strong>linear regression models</strong> is the <strong>least square</strong> method. The name of <strong>least square</strong> comes from the fact that the method minimizes the (square of the) error <span class="math inline">\(\mathbf{y} - \beta \mathbf{x}\)</span> by tuning the value of <span class="math inline">\(\beta\)</span>. The unique value of <span class="math inline">\(\beta\)</span> should match the data better than the others.</p>
<p>We will come back to the minimization point of <strong>least square</strong> in Ch[?]. At the moment, we can see how to find the best <span class="math inline">\(\beta\)</span> from the geometric point of view. The preassumbly relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is linear, so scaling the vector <span class="math inline">\(\mathbf{x}\)</span> by <span class="math inline">\(\beta\)</span> unit was supposed to get the vector <span class="math inline">\(\mathbf{y}\)</span>. Due to the contaminations<label for="tufte-sn-163" class="margin-toggle sidenote-number">163</label><input type="checkbox" id="tufte-sn-163" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">163</span> In our simulation, the contaminations were added by ten draws from a normal random variable <span class="math inline">\(\mathcal{N}(0,4)\)</span>)</span>, the vector <span class="math inline">\(\mathbf{y}\)</span> does not stay with <span class="math inline">\(\mathbf{x}\)</span> at the same plane. We know that when two points <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\mathbf{x}\)</span> are on different planes, the nearest point to <span class="math inline">\(\mathbf{x}\)</span> is the projection of <span class="math inline">\(\mathbf{y}\)</span> onto the plane of <span class="math inline">\(\mathbf{x}\)</span>. The value of <span class="math inline">\(\beta\)</span> comes from such a projection is the least square estimator, denoted by <span class="math inline">\(\hat{\beta}\)</span>.
In other words, we can <strong>estimate</strong> the parameter <span class="math inline">\(\beta\)</span> by projecting <span class="math inline">\(\mathbf{y}\)</span> onto the plane of <span class="math inline">\(\mathbf{x}\)</span>. By the <a href="#sub:Vector">projection formula</a> <a href="ch-vecMat.html#eq:proj">(10.4)</a>, we can deduce the estimator<span class="math display">\[\hat{\beta}=\frac{\langle\mathbf{x},\,\mathbf{y}\rangle}{\langle\mathbf{x},\,\mathbf{x}\rangle}\]</span>
and the estimated output (the projected output) <span class="math inline">\(\hat{\beta}\mathbf{x}\)</span>.
The difference between <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\hat{\beta}\mathbf{x}\)</span> is called the <em>residual</em>. The residual measures the discrepancy between the data and the estimated model. For any given input <span class="math inline">\(x\)</span>, one can also predict the output through <span class="math inline">\(\hat{\beta}x\)</span>.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb27-1" data-line-number="1"><span class="co"># least square estimate of beta (True beta is 5)</span></a>
<a class="sourceLine" id="cb27-2" data-line-number="2"><span class="kw">sum</span>(x<span class="op">*</span>y)<span class="op">/</span><span class="kw">sum</span>(x<span class="op">*</span>x)</a></code></pre></div>
<pre><code>## [1] 5.027272</code></pre>
</div>
<div id="sub:matrix" class="section level2">
<h2>
<span class="header-section-number">10.3</span> Matrix</h2>
<p>A <em>matrix</em> is a rectangular array, with rows and columns, of numbers.<label for="tufte-sn-164" class="margin-toggle sidenote-number">164</label><input type="checkbox" id="tufte-sn-164" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">164</span> Strictly speaking, it is a rectangular array of numbers of a <a href="">field</a>, see Ch[?].</span> If a <strong>matrix</strong> has <span class="math inline">\(m\)</span> rows and <span class="math inline">\(n\)</span> columns, then the size of the <strong>matrix</strong> is said to be <span class="math inline">\(m\times n\)</span> (read <span class="math inline">\(m\)</span> by <span class="math inline">\(n\)</span>). We use a boldface capital letter, e.g. <span class="math inline">\(\mathbf{A}\)</span>, to denote a matrix. We write<span class="math display">\[\mathbf{A}=[a_{ij}]_{m\times n}\;\mbox{for }1\leq i\leq m,\:1\leq j\leq n,\]</span>
where <span class="math inline">\(a_{ij}\)</span> is the <a href="ch-vecMat.html#sub:vec">entry</a> in location <span class="math inline">\((i,j)\)</span>, namely stored in the <span class="math inline">\(i\)</span>-th row and <span class="math inline">\(j\)</span>-th column of the matrix. In the expanded form, we write<span class="math display">\[\mathbf{A}=\left[\begin{array}{ccccc}
a_{11} &amp; \cdots &amp; a_{1j} &amp; \cdots &amp; a_{1n}\\
a_{21} &amp; \cdots &amp; a_{2j} &amp; \cdots &amp; a_{2n}\\
\vdots &amp;  &amp;  &amp;  &amp; \vdots\\
a_{i1} &amp; \cdots &amp; a_{ij} &amp; \cdots &amp; a_{in}\\
\vdots &amp;  &amp;  &amp;  &amp; \vdots\\
a_{m1} &amp; \cdots &amp; a_{mj} &amp; \cdots &amp; a_{mn}
\end{array}\right].\]</span>
As you can see, each column of the <span class="math inline">\(m\times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span> is an <span class="math inline">\(m\)</span>-<a href="ch-vecMat.html#sub:vec">vector</a> <span class="math inline">\(\mathbf{a}_{j}\)</span> for <span class="math inline">\(j=1,\dots n\)</span>. We can express the <strong>matrix</strong> as <span class="math display">\[\mathbf{A}=[\mathbf{a}_{1},\dots,\mathbf{a}_{n}],\]</span>
which is an <span class="math inline">\(n\)</span>-row <a href="ch-vecMat.html#sub:vec">vector</a> with <span class="math inline">\(m\)</span> column <a href="ch-vecMat.html#sub:vec">vectors</a> as <a href="ch-vecMat.html#sub:vec">entries</a>. If <span class="math inline">\(m=1\)</span>, then the <span class="math inline">\(1\times n\)</span> matrix is a row <a href="ch-vecMat.html#sub:vec">vector</a>; if <span class="math inline">\(n=1\)</span>, then this <span class="math inline">\(m\times1\)</span> matrix is a (column) <a href="ch-vecMat.html#sub:vec">vector</a>. If <span class="math inline">\(m=n\)</span>, the matrix is called a <em>squared matrix</em> of <em>order</em> <span class="math inline">\(n\)</span>. In figure <a href="ch-vecMat.html#fig:MatrixVis">10.7</a>, we can see that a <span class="math inline">\(3\times3\)</span> matrix is constructed by three 3-vectors.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:MatrixVis"></span>
<img src="fig/Part3/MatrixVis.png" alt="Matrix and vectors" width="100%"><!--
<p class="caption marginnote">-->Figure 10.7: Matrix and vectors<!--</p>-->
<!--</div>--></span>
</p>
<p>Any vector, either a row or a column, can be <em>transposed</em>, which means that a row vector turns into a column vector, and vice versa. Sometimes we define the vector by writing out its elements in the text as a row vector, then using the <strong>transpose</strong> operator to turn it into a standard column vector, e.g. <span class="math inline">\(\mathbf{a}=[a_{1},a_{2},a_{3}]^{\top}\)</span>. For an <span class="math inline">\(m\)</span>-columns vector <span class="math inline">\(\mathbf{a}\)</span>, its transposed vector is an <span class="math inline">\(m\)</span>-rows vector, denoted by <span class="math inline">\(\mathbf{a}^{\top}=[a_{1},\dots,a_{m}\)</span>.] The <a href="ch-vecMat.html#sub:vec">order</a> of the vector components is preserved. In addition, we can <strong>transpose</strong> a vector twice, and get back the same vector, i.e., <span class="math inline">\((\mathbf{a}^{\top})^{\top}=\mathbf{a}\)</span>.</p>
<p>With the <strong>transpose</strong> operation, we can express the <a href="ch-vecMat.html#sub:vec">inner product</a> <span class="math inline">\(\langle\mathbf{a},\,\mathbf{b}\rangle\)</span> as the product of <span class="math inline">\(\mathbf{a}^{\top}\)</span> and <span class="math inline">\(\mathbf{b}\)</span>, such that <span class="math display">\[\langle\mathbf{a},\,\mathbf{b}\rangle=\mathbf{a}^{\top}\mathbf{b}=[a_{1},\dots a_{m}]\left[\begin{array}{c}
b_{1}\\
\vdots\\
b_{m}
\end{array}\right]=\sum_{i=1}^{m}a_{i}b_{i},\]</span>
a sum of component-wise multiplications, each <span class="math inline">\(i\)</span>-th position of the row vector times the <span class="math inline">\(i\)</span>-th position of the column vector. Note that for the vector <span class="math inline">\(\mathbf{a}=[a_{1},\dots,a_{m}]^{\top}\)</span>, and <span class="math inline">\(\mathbf{b}^{\top}=[b_{1},\dots,b_{m}]\)</span>, <span class="math inline">\(\mathbf{a}\mathbf{b}^{\top}\)</span> is not a number but a matrix <span class="math inline">\(\mathbf{C}=[a_{i}b_{j}]_{ij}\)</span>.<label for="tufte-sn-165" class="margin-toggle sidenote-number">165</label><input type="checkbox" id="tufte-sn-165" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">165</span> In quantum mechanics, it is common to write the <a href="ch-vecMat.html#sub:vec">inner product</a> <span class="math inline">\(\langle\mathbf{a},\,\mathbf{b}\rangle\)</span> as <span class="math inline">\(\langle\mathbf{a}\,|\,\mathbf{b}\rangle\)</span>, and write <span class="math inline">\(\mathbf{a}\mathbf{b}^{\top}\)</span> as <span class="math inline">\(|\mathbf{a}\rangle\,\langle\mathbf{b}|\)</span> which is called the <em>outer product</em>. The vector <span class="math inline">\(\mathbf{a}\)</span> is denoted by <span class="math inline">\(|\mathbf{a}\rangle\)</span>, and its transport <span class="math inline">\(\mathbf{a}^{\top}\)</span> is denoted by <span class="math inline">\(\langle\mathbf{a}\,|\)</span>.</span></p>
<p>A matrix can also be <strong>transposed</strong>. The <strong>transpose</strong> of a matrix is the mirror image of the matrix across the diagonal line. The transpose of an <span class="math inline">\(m\times n\)</span> matrix <span class="math inline">\(\mathbf{A}=[a_{ij}]_{m\times n}\)</span> is denoted by <span class="math inline">\(\mathbf{A}^{\top}=[a_{ji}]_{n\times m}\)</span>, i.e.<span class="math display">\[\left[\begin{array}{cc}
1 &amp; 2\\
3 &amp; 4\\
5 &amp; 6
\end{array}\right]^{\top}=\left[\begin{array}{ccc}
1 &amp; 3 &amp; 5\\
2 &amp; 4 &amp; 6
\end{array}\right].\]</span>
We make the columns of <span class="math inline">\(\mathbf{A}\)</span> into rows in <span class="math inline">\(\mathbf{A}^{\top}\)</span> (or rows of <span class="math inline">\(\mathbf{A}\)</span> into columns in <span class="math inline">\(\mathbf{A}^{\top}\)</span>).</p>
<p>Let <span class="math inline">\(\mathbf{A}=[a_{ij}]_{m\times n}\)</span> and <span class="math inline">\(\mathbf{B}=[b_{ij}]_{m\times n}\)</span> be the matrices of the same size. Then the <strong>sum of the matrices</strong>, denoted by <span class="math inline">\(\mathbf{A}+\mathbf{B}\)</span>, results another <span class="math inline">\(m\times n\)</span> matrix: <span class="math display">\[\mathbf{A}+\mathbf{B}=\left[\begin{array}{ccc}
a_{11}+b_{11} &amp; \cdots &amp; a_{1n}+b_{1n}\\
\vdots &amp; \cdots &amp; \vdots\\
a_{m1}+b_{m1} &amp; \cdots &amp; a_{mn}+b_{mn}
\end{array}\right]=[a_{ij}+b_{ij}]_{m\times n}.\]</span>
The negative of the matrix <span class="math inline">\(\mathbf{A}\)</span>, denoted by <span class="math inline">\(-\mathbf{A}\)</span> is <span class="math inline">\(-\mathbf{A}=[-a_{ij}]_{m\times n}\)</span>. Thus the difference of <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span> is <span class="math inline">\(\mathbf{A}-\mathbf{B}=[a_{ij}-b_{ij}]_{m\times n}\)</span>. Notice that both matrices and vectors must be the same size before we attempt to add them. The product of a <a href="ch-vecMat.html#sub:vec">scalar</a> <span class="math inline">\(c\)</span> with the matrix <span class="math inline">\(\mathbf{A}\)</span>
is <span class="math inline">\(c\mathbf{A}=[ca_{ij}]_{m\times n}\)</span>. Because the matrix is simply constructed by vectors, the algebraic <a href="ch-vecMat.html#sub:vec">axioms of the vector</a>, namely <a href="ch-vecMat.html#sub:vec">addition</a> and <a href="ch-vecMat.html#sub:vec">scalar mutiplication</a>, also work for the matrix.<label for="tufte-sn-166" class="margin-toggle sidenote-number">166</label><input type="checkbox" id="tufte-sn-166" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">166</span>  <em>Addition</em>: (commutativity) <span class="math inline">\(\mathbf{A}+\mathbf{B}=\mathbf{B}+\mathbf{A}\)</span>,
(associativity) <span class="math inline">\(\mathbf{A}+(\mathbf{B}+\mathbf{C})=(\mathbf{A}+\mathbf{B})+\mathbf{C}\)</span>,
(additive inverse) <span class="math inline">\(\mathbf{A}+(-1)\mathbf{A}=0\)</span>.
<em>Scalar multiplication</em>: (associativity) <span class="math inline">\(k(l\mathbf{A})=(kl)\mathbf{A}\)</span>,
(distributivity) <span class="math inline">\((k+l)\mathbf{A}=k\mathbf{A}+l\mathbf{A}\)</span>, <span class="math inline">\(k(\mathbf{A}+\mathbf{B})=k\mathbf{A}+k\mathbf{B}\)</span>.</span></p>
<p>While matrix multiplication by a scalar and matrix addition are rather straightforward, the matrix-matrix multiplication may not be so. It is better to consider first the matrix-vector multiplication.</p>
<p>Since any <strong>matrix</strong> can be expressed as a row vector of column vectors, and since the inner product rule also holds for both types of vectors, we can express the matrix-vector multiplication in terms of <a href="ch-vecMat.html#sub:vec">inner products</a>. For an <span class="math inline">\(m\times n\)</span> matrix <span class="math inline">\(\mathbf{A}=[\mathbf{a}_{1},\dots,\mathbf{a}_{n}]\)</span>
and an <span class="math inline">\(m\)</span>-vector <span class="math inline">\(\mathbf{b}\)</span>, the inner product between <span class="math inline">\(\mathbf{b}\)</span> and any <span class="math inline">\(\mathbf{a}_{j}\)</span> for <span class="math inline">\(j=1,\dots n\)</span> follows the previous definition such that <span class="math inline">\(\langle\mathbf{b},\,\mathbf{a}_{j}\rangle=\sum_{i=1}^{m}b_{i}a_{ij}\)</span>. As the matrix <span class="math inline">\(\mathbf{A}\)</span> consists of <span class="math inline">\(n\)</span> components of <span class="math inline">\(m\)</span>-vector <span class="math inline">\(\mathbf{a}_{j}\)</span>, we need to have the component-wise inner products for all <span class="math inline">\(\mathbf{a}_{j}\)</span>. The result is a row <span class="math inline">\(n\)</span>-vector <span class="math inline">\([\sum_{i=1}^{m}b_{i}a_{i1},\dots,\sum_{i=1}^{m}b_{i}a_{in}]\)</span>. We can write a compact expression using the transpose operation <span class="math display">\[\mathbf{b}^{\top}\mathbf{A}=\left[\mathbf{b}^{\top}\mathbf{a}_{1},\dots,\mathbf{b}^{\top}\mathbf{a}_{n}\right].\]</span>
which reads “vector <span class="math inline">\(\mathbf{b}\)</span> times matrix <span class="math inline">\(\mathbf{A}\)</span>.” For an <span class="math inline">\(n\)</span>-vector <span class="math inline">\(\mathbf{x}\)</span>, the mutiplication results in<span class="math display">\[\mathbf{A}\mathbf{x}=[\mathbf{a}_{1},\dots\mathbf{a}_{n}]\left[\begin{array}{c}
x_{1}\\
\vdots\\
x_{n}
\end{array}\right]=\mathbf{a}_{1}x_{1}+\cdots+\mathbf{a}_{n}x_{n}\]</span></p>
<p>which is a <a href="ch-vecMat.html#sub:vec">linear combination</a> of <span class="math inline">\(\mathbf{a}_{j}\)</span>, <span class="math inline">\(j=1,\dots,n\)</span>. We read “matrix <span class="math inline">\(\mathbf{A}\)</span> times vector <span class="math inline">\(\mathbf{x}\)</span>”. Note that <span class="math inline">\(\mathbf{A}\mathbf{x}\)</span> is a column <span class="math inline">\(m\)</span>-vector while <span class="math inline">\(\mathbf{b}^{\top}\mathbf{A}\)</span> is a row <span class="math inline">\(n\)</span>-vector. As you can see, the <a href="ch-vecMat.html#sub:vec">inner product</a> restricts the multiplication rule to the case of two vectors with an identical size.<label for="tufte-sn-167" class="margin-toggle sidenote-number">167</label><input type="checkbox" id="tufte-sn-167" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">167</span> For <span class="math inline">\(\mathbf{x}\mathbf{A}\)</span>, the size of the row of <span class="math inline">\(\mathbf{A}\)</span>
has to equal to the size of <span class="math inline">\(\mathbf{x}\)</span>. For <span class="math inline">\(\mathbf{A}\mathbf{x}\)</span>, the size of the column of <span class="math inline">\(\mathbf{A}\)</span> has to equal to the size of <span class="math inline">\(\mathbf{x}\)</span>.</span> Figure <a href="ch-vecMat.html#fig:MatrixVec">10.8</a> illustrates how to compute the multiplication of <span class="math inline">\(\mathbf{A}\mathbf{x}\)</span> by using row-vectors of <span class="math inline">\(\mathbf{A}\)</span>, then calculating the inner products between the row-vectors and <span class="math inline">\(\mathbf{x}\)</span>.<label for="tufte-sn-168" class="margin-toggle sidenote-number">168</label><input type="checkbox" id="tufte-sn-168" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">168</span> The numerical entries come from Nine Chapters on the Mathematical Art. In the chapter of the system of equations (Chapter 8), it gives 18 problems and one of them (problem 17 about the costs of a sheep, a dog, a chick and a rabbit) can be expressed in this multiplication.</span></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:MatrixVec"></span>
<img src="fig/Part3/MatrixVecM.gif" alt="Illustration of the rule of matrix-vector multiplication" width="100%"><!--
<p class="caption marginnote">-->Figure 10.8: Illustration of the rule of matrix-vector multiplication<!--</p>-->
<!--</div>--></span>
</p>
<p><strong>Matrix-vector</strong> notation can help us to simplify the operations for linear systems. Recall that a single linear equation <span class="math inline">\(2x_{1}-3x_{2}+4x_{3}=5\)</span> can be expressed as an inner product: <span class="math display">\[[2,-3,4]\left[\begin{array}{c}
x_{1}\\
x_{2}\\
x_{3}
\end{array}\right]=5.\]</span>
The <em>linear system</em> is a collection of linear equations. Consider the following system:
<span class="math display" id="eq:linearSys">\[\begin{equation}
\begin{array}{cc}
x_{1} &amp; =y_{1},\\
-x_{1}+x_{2} &amp; =y_{2},\\
-x_{2}+x_{3} &amp; =y_{3}.
\end{array}
\tag{10.5}
\end{equation}\]</span>
For three vectors <span class="math display">\[\mathbf{a}_{1}=\left[\begin{array}{c}
1\\
-1\\
0
\end{array}\right],\:\mathbf{a}_{2}=\left[\begin{array}{c}
0\\
1\\
-1
\end{array}\right],\:\mathbf{a}_{3}=\left[\begin{array}{c}
0\\
0\\
1
\end{array}\right],\]</span>
their <a href="ch-vecMat.html#sub:vec">linear combinations</a> <span class="math inline">\(x_{1}\mathbf{a}_{1}+x_{2}\mathbf{a}_{2}+x_{3}\mathbf{a}_{3}\)</span> gives the system <a href="ch-vecMat.html#eq:linearSys">(10.5)</a> <span class="math display">\[x_{1}\left[\begin{array}{c}
1\\
-1\\
0
\end{array}\right]+x_{2}\left[\begin{array}{c}
0\\
1\\
-1
\end{array}\right],+x_{3}\left[\begin{array}{c}
0\\
0\\
1
\end{array}\right]=\left[\begin{array}{c}
x_{1}\\
x_{2}-x_{1}\\
x_{3}-x_{2}
\end{array}\right]=\mathbf{y}.\]</span>
We can rewrite this <a href="ch-vecMat.html#sub:vec">combination</a> using the matrix <span class="math inline">\(\mathbf{A}\)</span> whose columns are the vectors <span class="math inline">\(\mathbf{a}_{1}\)</span>, <span class="math inline">\(\mathbf{a}_{2}\)</span>, and <span class="math inline">\(\mathbf{a}_{3}\)</span>. The vector <span class="math inline">\(\mathbf{y}\)</span> is the result of the mutiplication between matrix <span class="math inline">\(\mathbf{A}\)</span> and vector <span class="math inline">\(\mathbf{x}\)</span><label for="tufte-sn-169" class="margin-toggle sidenote-number">169</label><input type="checkbox" id="tufte-sn-169" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">169</span> Solving this system of equations is a simple calculation of inverting the matrix <span class="math inline">\(\mathbf{A}\)</span> to have the solution <span class="math inline">\(\mathbf{x}=\mathbf{A}^{-1}\mathbf{y}\)</span>. We will discuss the matrix inversion in section [?].</span>: <span class="math display">\[\begin{align*}
\mathbf{A}\mathbf{x}&amp;=\underset{3\times3}{\underbrace{\left[\begin{array}{ccc}
1 &amp; 0 &amp; 0\\
-1 &amp; 1 &amp; 0\\
0 &amp; -1 &amp; 1
\end{array}\right]}}\underset{3\times1}{\underbrace{\left[\begin{array}{c}
x_{1}\\
x_{2}\\
x_{3}
\end{array}\right]}}\\ &amp;=\left[\mathbf{a}_{1},\mathbf{a}_{2},\mathbf{a}_{3}\right]\left[\begin{array}{c}
x_{1}\\
x_{2}\\
x_{3}
\end{array}\right]=x_{1}\mathbf{a}_{1}+x_{2}\mathbf{a}_{2}+x_{3}\mathbf{a}_{3}=\mathbf{y}.\end{align*}\]</span></p>
<p>Let <span class="math inline">\(\mathbf{A}\)</span> be an <span class="math inline">\(m\times n\)</span> matrix and <span class="math inline">\(\mathbf{B}\)</span>
be an <span class="math inline">\(n\times p\)</span> matrix. The general <em>matrix-matrix multiplication</em> rule of <span class="math inline">\(\mathbf{A}\mathbf{B}\)</span> can also be expressed in the <a href="ch-vecMat.html#sub:vec">inner product</a> way: taking the inner product of each row of <span class="math inline">\(\mathbf{A}\)</span> with each column of <span class="math inline">\(\mathbf{B}\)</span>. The product <span class="math inline">\(\mathbf{A}\mathbf{B}\)</span> results an <span class="math inline">\(m\times p\)</span> matrix:<span class="math display">\[\underset{m\times n}{\underbrace{\mathbf{A}}}\underset{n\times p}{\underbrace{\mathbf{B}}}=\underset{m\times p}{\underbrace{\mathbf{C}}}\]</span> where the entry <span class="math inline">\(c_{ij}\)</span> of <span class="math inline">\(\mathbf{C}\)</span> is the <span class="math inline">\(i\)</span>-th row of <span class="math inline">\(\mathbf{A}\)</span> multiplying the <span class="math inline">\(j\)</span>-th column of <span class="math inline">\(\mathbf{B}\)</span>. The complete form of this <strong>matrix-matrix multiplication</strong> is as follows:<span class="math display">\[\left[\begin{array}{ccc}
a_{11} &amp; \cdots &amp; a_{1n}\\
\vdots &amp; \cdots &amp; \vdots\\
a_{m1} &amp; \cdots &amp; a_{mn}
\end{array}\right]\left[\begin{array}{ccc}
b_{11} &amp; \cdots &amp; b_{1n}\\
\vdots &amp; \cdots &amp; \vdots\\
b_{m1} &amp; \cdots &amp; b_{mn}
\end{array}\right]=\left[\begin{array}{ccc}
c_{11} &amp; \cdots &amp; c_{1p}\\
\vdots &amp; \cdots &amp; \vdots\\
c_{m1} &amp; \cdots &amp; c_{mp}
\end{array}\right]\]</span>
where <span class="math inline">\([c_{ij}]_{m\times p}=[\sum_{k=1}^{n}a_{ik}b_{kj}]\)</span>. Figure <a href="ch-vecMat.html#fig:MatrixMatrix">10.9</a> gives a specific example.</p>
<div class="figure">
<span id="fig:MatrixMatrix"></span>
<p class="caption marginnote shownote">
Figure 10.9: Matrix-matrix multiplication
</p>
<img src="fig/Part3/MatrixMatrixM.gif" alt="Matrix-matrix multiplication" width="100%">
</div>
<p>Here are the <em>laws of the matrix multiplication</em>. Let <span class="math inline">\(\mathbf{A}=[a_{ij}]_{m\times n}\)</span>, <span class="math inline">\(\mathbf{B}=[b_{ij}]_{n\times p}\)</span>, <span class="math inline">\(\mathbf{C}=[c_{ij}]_{n\times p}\)</span>, <span class="math inline">\(\mathbf{D}=[d_{ij}]_{p\times q}\)</span>, and <span class="math inline">\(c\in\mathbb{R}\)</span>.</p>
<ul>
<li>
<em>Identity</em> : <span class="math inline">\(\mathbf{A}\mathbf{I}_{n}=\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{I}_{m}\mathbf{A}=\mathbf{A}\)</span>, for the <em>identity matrix</em> <span class="math display">\[\mathbf{I}_{k}=\left[\begin{array}{ccccc}
1 &amp; 0 &amp; \cdots &amp;  &amp; 0\\
0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0\\
\vdots &amp;  &amp; \ddots\\
0 &amp; \cdots &amp;  &amp; 1 &amp; 0\\
0 &amp; 0 &amp; \cdots &amp; 0 &amp; 1
\end{array}\right]=[\delta_{ij}]_{k\times k}\]</span>
where <span class="math inline">\(\delta_{ij}=1\)</span> if <span class="math inline">\(i=j\)</span>, and <span class="math inline">\(\delta_{ij}=0\)</span> otherwise.</li>
<li>
<em>Associativity</em> : <span class="math inline">\((\mathbf{A}\mathbf{B})\mathbf{D}=\mathbf{A}(\mathbf{B}\mathbf{D})\)</span>.</li>
<li>
<em>Associativity for scalar</em> : <span class="math inline">\(c(\mathbf{A}\mathbf{B})=(c\mathbf{A})\mathbf{B}=\mathbf{A}(c\mathbf{B})\)</span>.</li>
<li>
<em>Distributivity</em> : <span class="math inline">\(\mathbf{A}(\mathbf{B}+\mathbf{C})=\mathbf{A}\mathbf{B}+\mathbf{A}\mathbf{C}\)</span>.</li>
<li>
<em>Transpose rule</em> : <span class="math inline">\((\mathbf{A}^{\top})^{\top}=\mathbf{A}\)</span>, <span class="math inline">\((\mathbf{A}\mathbf{B})^{\top}=\mathbf{B}^{\top}\mathbf{A}^{\top}\)</span>, <span class="math inline">\((\mathbf{B}+\mathbf{C})^{\top}=\mathbf{B}^{\top}+\mathbf{C}^{\top}\)</span>, <span class="math inline">\((c\mathbf{A})^{\top}=c\mathbf{A}^{\top}\)</span>.</li>
</ul>
<div class="solution">
<p class="solution-begin">
Proof <span id="sol-start-35" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-35', 'sol-start-35')"></span>
</p>
<div id="sol-body-35" class="solution-body" style="display: none;">
<p>Associativity comes from the scalar multiplication in the <a href="ch-vecMat.html#sub:vec">axioms of vectors</a>.
Distributivity: Let <span class="math inline">\(\mathbf{B}=[\mathbf{b}_{1},\dots,\mathbf{b}_{p}]\)</span>, <span class="math inline">\(\mathbf{C}=[\mathbf{c}_{1},\dots,\mathbf{c}_{p}]\)</span>. The law <span class="math inline">\(\mathbf{A}(\mathbf{B}+\mathbf{C})=\mathbf{A}\mathbf{B}+\mathbf{A}\mathbf{C}\)</span> is proved a column at a time, <span class="math inline">\(\mathbf{A}(\mathbf{b}_{i}+\mathbf{d}_{i})=\mathbf{A}\mathbf{b}_{i}+\mathbf{A}\mathbf{d}_{i}\)</span> for <span class="math inline">\(i=1,\dots,p\)</span>.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>Notice that you can only multiply two matrices <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span> provided that their dimensions are compatible, which means the number of columns of <span class="math inline">\(\mathbf{A}\)</span> equals the number of rows of <span class="math inline">\(\mathbf{B}\)</span>. Also, notice that the <a href="ch-vecMat.html#sub:vec">commutativity</a> is broken for matrix multiplication in general. That is <span class="math inline">\(\mathbf{A}\mathbf{B}\neq\mathbf{B}\mathbf{A}\)</span>. In fact, <span class="math inline">\(\mathbf{B}\mathbf{A}\)</span> may not even make sense because of incompatible sizes <span class="math inline">\(p\neq m\)</span>. Even when <span class="math inline">\(\mathbf{A}\mathbf{B}\)</span> and <span class="math inline">\(\mathbf{B}\mathbf{A}\)</span> both make sense and are of the same size, in general we do not have the <a href="ch-vecMat.html#sub:vec">commutativity</a>. Thus, for matrix multiplications, the order matters.<label for="tufte-sn-170" class="margin-toggle sidenote-number">170</label><input type="checkbox" id="tufte-sn-170" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">170</span> The violation of the commutativity reveals a deeper root in the abstract layer. For an <span class="math inline">\(m\times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span>, the <strong>matrix-vector multiplication</strong> <span class="math inline">\(\mathbf{A}\mathbf{x}=\mathbf{y}\)</span> product another <span class="math inline">\(m\)</span>-vector <span class="math inline">\(\mathbf{y}\)</span>. So we can think of <span class="math inline">\(\mathbf{A}:\mathbb{R}^{n}\mapsto\mathbb{R}^{m}\)</span> as a linear function (or mapping) from <span class="math inline">\(n\)</span>-vector <span class="math inline">\(\mathbf{x}\)</span> to <span class="math inline">\(m\)</span>-vector <span class="math inline">\(\mathbf{y}\)</span>. That is, <span class="math inline">\(\mathbf{A}\mathbf{x}=f(\mathbf{x})\)</span>. Similarly, for another <span class="math inline">\(n\times p\)</span> matrix <span class="math inline">\(\mathbf{B}\)</span>, <span class="math inline">\(\mathbf{B}:\mathbb{R}^{n}\mapsto\mathbb{R}^{p}\)</span> is a linear function <span class="math inline">\(g(\cdot)\)</span>. The <a href="sub-set-theory.html#sub:func">composition</a> <span class="math inline">\(f\circ g=\mathbf{A}\mathbf{B}\)</span> is generally not commutative, namely <span class="math inline">\(f\circ g\neq g\circ f\)</span>. We will discuss these abstract objects in ch[?].</span></p>
</div>
<div id="sub:linearSys" class="section level2">
<h2>
<span class="header-section-number">10.4</span> Example: Linear Systems</h2>
<p>The algebra of <a href="ch-vecMat.html#sub:vec">vectors</a> and <a href="#sub:maxtrix">matrices</a> is called the <em>linear algebra</em>. The central problem of linear algebra is to solve a <em>linear system of equations</em>. A <strong>linear system</strong> of <span class="math inline">\(m\)</span> equations in the <span class="math inline">\(n\)</span> unknowns <span class="math inline">\(x_{1},\dots,x_{n}\)</span> has the form
<span class="math display">\[
\begin{align*}
a_{11}x_{1}+\cdots+a_{1n}x_{n}  &amp;=y_{1},\\
a_{21}x_{1}+\cdots+a_{2n}x_{n}  &amp;=y_{2},\\
\vdots\qquad    \quad &amp; \vdots\\
a_{m1}x_{1}+\cdots+a_{mn}x_{n}  &amp;=y_{n}.
\end{align*}
\]</span>
The coefficients of this system can form an <span class="math inline">\(m\times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span>, the variables can form an <span class="math inline">\(n\)</span>-vector <span class="math inline">\(\mathbf{x}\)</span>, and the output can form an <span class="math inline">\(m\)</span>-vector <span class="math inline">\(\mathbf{y}\)</span>. The expression <span class="math inline">\(\mathbf{A}\mathbf{x}=\mathbf{y}\)</span> is the standard form of a <strong>linear system</strong>. We will see that many vital problems belong to such a form. Solving the <strong>linear system</strong> is about the inversion: to find the input <span class="math inline">\(\mathbf{x}\)</span> that gives the desired output <span class="math inline">\(\mathbf{y}=\mathbf{A}\mathbf{x}\)</span>. We will discuss the matrix inversion in ch [?]. Now let’s focus on how to model the phenomena within this linear system framework.</p>
<p><span class="newthought">Finite Difference </span></p>
<p>Consider the previous system <a href="ch-vecMat.html#eq:linearSys">(10.5)</a>. The system can be written as
<span class="math display">\[\begin{split}x_{1} &amp; =y_{1}\\
x_{2}-x_{1} &amp; =y_{2}\\
x_{3}-x_{2} &amp; =y_{3}
\end{split}
\quad\mbox{or }\mathbf{A}\mathbf{x}=\mathbf{y}\mbox{ with }\mathbf{A}=\left[\begin{array}{ccc}
1 &amp; 0 &amp; 0\\
-1 &amp; 1 &amp; 0\\
0 &amp; -1 &amp; 1
\end{array}\right]\]</span>
where <span class="math inline">\(\mathbf{A}\)</span> is a special matrix called the <em>difference matrix</em>. The vector <span class="math inline">\(\mathbf{A}\mathbf{x}\)</span> produces a vector of differences of consecutive entries of <span class="math inline">\(\mathbf{x}\)</span>.<label for="tufte-sn-171" class="margin-toggle sidenote-number">171</label><input type="checkbox" id="tufte-sn-171" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">171</span> The first one can be thought of as <span class="math inline">\(x_{1}-x_{0}\)</span> with <span class="math inline">\(x_{0}=0\)</span>.</span> The differences <span class="math inline">\(\mathbf{A}\mathbf{x}\)</span>
in fact is a discrete counterpart of the derivative <span class="math inline">\(\mbox{d}x(t)/\mbox{d}t\)</span>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:EquiDis"></span>
<img src="fig/Part3/EquiDis.png" alt="Equidistant time point" width="100%"><!--
<p class="caption marginnote">-->Figure 10.10: Equidistant time point<!--</p>-->
<!--</div>--></span>
</p>
<p>To see this argument, we can consider the <span class="math inline">\(n\)</span>-vector <span class="math inline">\(\mathbf{x}=[x_{1},\dots,x_{n}]^{\top}\)</span> as <span class="math inline">\(n\)</span> discrete observations of <span class="math inline">\(x(t)\)</span> at <span class="math inline">\(n\)</span>
equidistant time points <span class="math inline">\(t_{1},\dots,t_{n}\)</span>. See fig <a href="ch-vecMat.html#fig:EquiDis">10.10</a>. We can approximate <span class="math inline">\(\mbox{d}x(t)/\mbox{d}t\)</span> by <span class="math display">\[\frac{\mbox{d}x(t)}{\mbox{d}t}\approx\frac{x(t+\Delta t)-x(t)}{\Delta t}\approx x_{t+1}-x_{t}\mbox{ when }\Delta t=1.\]</span>
Then the <a href="ch-DE.html#ch:DE">differential equation</a> <span class="math inline">\(\mbox{d}x(t)/\mbox{d}t=y(t)\)</span> is approximated by <span class="math inline">\(\mathbf{A}\mathbf{x}=\mathbf{y}\)</span> where <span class="math inline">\(\mathbf{A}\)</span>
is <span class="math display">\[ \mathbf{A}=\left[\begin{array}{ccccc}
1 &amp; 0 &amp; \cdots &amp;  &amp; 0\\
-1 &amp; 1 &amp; 0 &amp; \cdots\\
\vdots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \vdots\\
0 &amp; \cdots &amp; -1 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; \cdots &amp; -1 &amp; 1
\end{array}\right],\]</span> an <span class="math inline">\(n\times n\)</span> general <strong>difference matrix</strong>,
and <span class="math inline">\(\mathbf{A}\mathbf{x}=\mathbf{y}=[x_{1}-x_{0},\dots,x_{n}-x_{n-1}]^{\top}\)</span> gives an <span class="math inline">\(n\)</span>-vector of consecutive differences.</p>
<p><span class="newthought">Linear dynamical system </span></p>
<p>The <strong>difference matrix</strong> is a numerical device to discretize a single dynamical variable <span class="math inline">\(x(t)\)</span>. In many cases, the interest is to model a whole system of dynamical variables <span class="math inline">\(x_1(t), x_2(t),\dots,x_n(t)\)</span> with either continuous or discrete time setting. We can pursuit such an interest by using vectors and matrices.</p>
<ul>
<li>A <em>continuous (time) linear dynamical system</em> is a sequence of the vector of <a href="ch-DE.html#sub:EulerScheme">state variables</a>, called the <em>state vector</em> <span class="math inline">\(\mathbf{x}(t)=[x_{1}(t), x_{2}(t),\dots,x_{n}(t)]\)</span>. The sequence is defined by an initial vector <span class="math inline">\(\mathbf{x}(0)\)</span> and by the <a href="ch-DE.html#ch:DE">dynamical law</a> <span class="math display">\[\frac{\mbox{d}\mathbf{x}(t)}{\mbox{d}t}=\mathbf{A}(t)\mathbf{x}(t)+\mathbf{u}(t) ,\;\mbox{for }t&gt;0.\]</span>
The <a href="ch-vecMat.html#sub:matrix">square matrix</a> <span class="math inline">\(\mathbf{A}(t)\)</span> is called the <em>transition matrix</em> of the system, and the vector <span class="math inline">\(\mathbf{u}(t)\)</span> is called the <em>controlled vector</em> (or <em>input vector</em>) of the system as <span class="math inline">\(u_{i}(t)\)</span> in <span class="math inline">\(\mathbf{u}(t)\)</span> is defined as the <a href="sub-inferknow.html#determinism">controlled variable</a>. When <span class="math inline">\(\mathbf{u}(t)=0\)</span> for all <span class="math inline">\(t\)</span>, the system becomes a <em>homogeneous dynamical system</em>.</li>
</ul>
<p>In pratice, the discrete version of the <strong>dynamical system</strong> appears more often in a wide variety of disciplines.</p>
<ul>
<li>A <em>discrete (time) linear dynamical system</em> is a sequence of the <em>state</em> vector <span class="math inline">\(\mathbf{x}_{t}=[x_{1,t}, x_{2,t},\dots,x_{n,t}]\)</span>, given the initial vector <span class="math inline">\(\mathbf{x}_{0}\)</span> and the <a href="ch-DE.html#ch:DE">dynamical law</a>
<span class="math display" id="eq:dynModel">\[
\begin{equation}
\mathbf{x}_{t}=\mathbf{A}_t\mathbf{x}_{t-1}+\mathbf{u}_{t},\;\mbox{for }t=0,1,\dots,
\tag{10.6}
\end{equation}
\]</span>
</li>
</ul>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:StateSpace"></span>
<img src="fig/Part3/stateSpace.gif" alt="Dynamics of state and controlled variables" width="100%"><!--
<p class="caption marginnote">-->Figure 10.11: Dynamics of state and controlled variables<!--</p>-->
<!--</div>--></span>
</p>
<p>Figure <a href="ch-vecMat.html#fig:StateSpace">10.11</a> illustrates the generation of the sequence of the <strong>state</strong> and <strong>controlled vector</strong>. The equation <a href="ch-vecMat.html#eq:dynModel">(10.6)</a> above is also called the <em>dynamics</em> or <em>update equation</em>, since it gives us the next <strong>state vector</strong>, i.e., <span class="math inline">\(\mathbf{x}_{t+1}\)</span>, as a function of the current <strong>state vector</strong> <span class="math inline">\(\mathbf{x}_{t}\)</span>. At the moment, we assume that the transition matrix does not depend on <span class="math inline">\(t\)</span>, in which case the linear dynamical system is called <em>time-invariant</em>, namely <span class="math inline">\(\mathbf{A}(t)=\mathbf{A}\)</span> or <span class="math inline">\(\mathbf{A}_t=\mathbf{A}\)</span>.</p>
<p>There are many variations on and extensions of the basic linear dynamical system model <a href="ch-vecMat.html#eq:dynModel">(10.6)</a>, some of which we will encounter later. Now let’s consider one of the simplest cases <span class="math inline">\(\mathbf{x}_{t}=\mathbf{A}\mathbf{x}_{t-1}\)</span>. If there is a vector <span class="math inline">\(\mathbf{x}^*\)</span> satisfying <span class="math inline">\(\mathbf{x}^*=\mathbf{A}\mathbf{x}^*\)</span>, then <span class="math inline">\(\mathbf{x}^*\)</span> gives the <a href="ch-DE.html#sub:EulerScheme">fixed points</a> of the system. The vector is also known as the <em>stationary (state) vector</em> for <span class="math inline">\(\mathbf{A}\)</span>.</p>
<p>The control or input <span class="math inline">\(\mathbf{u}_{t}\)</span> is in fact also a <strong>state vector</strong>, but it is a state vector which the controller can manipulate. For example, in the system of macroeconomics, the order or move in a monetary supply can be thought as a control. In the system of automobile, the drive of a vehicle is a control. These controls can be qunatified in some representations similar to <a href="ch-vecMat.html#eq:dynModel">(10.6)</a>. These are called the <em>linear control</em> models. In these models, the <em>state feedback control</em> means that <strong>state</strong> <span class="math inline">\(\mathbf{x}_{t}\)</span> is measured, and the <strong>control</strong> <span class="math inline">\(\mathbf{u}_{t}\)</span> is a <a href="ch-vecMat.html#sub:linearity">linear function</a> of the state, namely <span class="math inline">\(\mathbf{u}_{t}=\mathbf{K}\mathbf{x}_{t}\)</span>, where <span class="math inline">\(\mathbf{K}\)</span>
is called the <em>state-feedback matrix</em>.<label for="tufte-sn-172" class="margin-toggle sidenote-number">172</label><input type="checkbox" id="tufte-sn-172" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">172</span> The term <strong>feedback</strong> refers to the idea that when the state <span class="math inline">\(\mathbf{x}_t\)</span> is measured, it (after multiplying by <span class="math inline">\(\mathbf{K}\)</span>) feeds back into the system via the input.</span> The whole system becomes <span class="math display">\[\mathbf{x}_{t}=\mathbf{A}\mathbf{x}_{t-1}+\mathbf{K}\mathbf{x}_{t-1}=(\mathbf{A}+\mathbf{K})\mathbf{x}_{t-1}\]</span>
which is similar to the system without the control.<label for="tufte-sn-173" class="margin-toggle sidenote-number">173</label><input type="checkbox" id="tufte-sn-173" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">173</span> The reason is that the <strong>linear control</strong> creates a loop (the <a href="sub-inferknow.html#determinism">successor function</a>, see section <a href="sub-inferknow.html#determinism">4.4</a>) where the control becomes “invisible” if we only concentrate on the transition of the state vector <span class="math inline">\(\mathbf{x}_{t}\)</span>. In this loop, the state affects the control, and the control affects the next state, but the <strong>control</strong> is an intermediate input <strong>state</strong> that is “invisible” both at the initial and at the terminated stage of the loop.</span></p>
<p><span class="newthought">Linear simultaneous system </span></p>
<p>The <strong>network system</strong> is another widely used <strong>linear system</strong>.</p>
<ul>
<li>
<em>Network</em> (or <em>graph</em>) : A <strong>network</strong> is denoted as <span class="math inline">\(\mathbb{G}=(\mathcal{V},\mathcal{E})\)</span> consisting of a set of <em>vertices</em> <span class="math inline">\(\mathcal{V}\)</span> and a set of <em>edges</em> <span class="math inline">\(\mathcal{E}\)</span>. The <strong>edge</strong> describes whether or not a <strong>pair</strong> of <strong>vertices</strong> is connected. The set of edges <span class="math inline">\(\mathcal{E}\)</span>
is a set of <em>pairs</em> of elements in <span class="math inline">\(\mathcal{V}\)</span> such that <span class="math inline">\(\mathcal{E}\subset\mathcal{V}\times\mathcal{V}\)</span>. If the <strong>pairs</strong> in <span class="math inline">\(\mathcal{E}\subset\mathcal{V}\times\mathcal{V}\)</span> are an <a href="sub-set-theory.html#sub:order">ordered</a> <strong>pair</strong>,<label for="tufte-sn-174" class="margin-toggle sidenote-number">174</label><input type="checkbox" id="tufte-sn-174" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">174</span> Suppose <span class="math inline">\(i,j\in\mathcal{V}\)</span>. Then an edge can be found from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span>, or from <span class="math inline">\(j\)</span> to <span class="math inline">\(i\)</span>. The pair has no direction if the edge from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span> does not differ from the edge from <span class="math inline">\(j\)</span> to <span class="math inline">\(i\)</span>. Otherwise, it is an <strong>order pair</strong>.</span> then <span class="math inline">\(\mathbb{G}=(\mathcal{V},\mathcal{E})\)</span> is a <em>directed network</em> (or <em>directed graph</em>), and the edges are called <em>directed edges</em>.</li>
</ul>
<p>Two <strong>vertices</strong> are <em>neighbors</em> if there is an <strong>edge</strong> joining them. For vertices <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, write <span class="math inline">\(i\sim j\)</span> if <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are <strong>neighbors</strong>.<label for="tufte-sn-175" class="margin-toggle sidenote-number">175</label><input type="checkbox" id="tufte-sn-175" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">175</span> Note that the symbol <span class="math inline">\(\sim\)</span> means the <a href="sub-set-theory.html#sub:order">equivalence relation</a> such that it satisfies <a href="sub-set-theory.html#sub:order">reflexivity</a>, <a href="sub-set-theory.html#sub:order">symmetricity</a>, and <a href="sub-set-theory.html#sub:order">transitivity</a></span>. The <em>degree</em> of the vertex <span class="math inline">\(v\in\mathcal{V}\)</span>, <span class="math inline">\(\mbox{deg}(v)\)</span>, is the number of <strong>neighbors</strong> of <span class="math inline">\(v\)</span>, also known as the number of edges incident to that vertex <span class="math inline">\(v\)</span>.<label for="tufte-sn-176" class="margin-toggle sidenote-number">176</label><input type="checkbox" id="tufte-sn-176" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">176</span> A <em>weighted network</em> (or graph) is a network with a positive number (or weight) <span class="math inline">\(w(i,j)\)</span> assigned to each <strong>edge</strong>.</span></p>
<p>The information about our <strong>network</strong> (or <strong>graph</strong>) can be stored in its <strong>adjacency matrix</strong>. The <em>adjacency matrix</em> is a <a href="ch-vecMat.html#sub:matrix">square matrix</a> whose rows and columns are indexed by the <strong>vertices</strong> of the <strong>network</strong> <span class="math inline">\(\mathbb{G}=(\mathcal{V},\mathcal{E})\)</span>, and whose <span class="math inline">\((i,j)\)</span>-th entry is the number of <strong>edges</strong> going from vertex <span class="math inline">\(i\)</span> to vertex <span class="math inline">\(j\)</span>. When the entry is zero, there is no connection.</p>
<p>For a network with vertex set <span class="math inline">\(\mathcal{V}=\{\mbox{red},\mbox{ green},\mbox{ blue}\}\)</span>, and the edge set <span class="math inline">\(\mathcal{E}=\{\{\mbox{red},\mbox{green}\},\{\mbox{red, blue}\}\}\)</span>, the <strong>adjacency matrix</strong> <span class="math inline">\([a_{ij}]_{3\times3}\)</span> is <span class="math display">\[\left[\begin{array}{ccc}
0 &amp; 1 &amp; 1\\
1 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0
\end{array}\right]\]</span></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:adjMatrix"></span>
<img src="fig/Part3/adjMatrix.gif" alt="Somr adjacency matrices for three, four, five elements" width="100%"><!--
<p class="caption marginnote">-->Figure 10.12: Somr adjacency matrices for three, four, five elements<!--</p>-->
<!--</div>--></span>
</p>
<p>where we assume the position one is red, two is green, and three is blue so that <span class="math inline">\(a_{12}=a_{21}=1\)</span>, <span class="math inline">\(a_{13}=a_{31}=1\)</span>. The <strong>degree of vertex</strong> for red is <span class="math inline">\(2\)</span>, and <span class="math inline">\(\mbox{deg}(\mbox{blue})=\mbox{deg}(\mbox{green})=1\)</span>. Recall that a <a href="ch-vecMat.html#sub:matrix">square matrix</a> <span class="math inline">\(\mathbf{A}\)</span> is <a href="ch-vecMat.html#sub:matrix">symmetric</a> if <span class="math inline">\(\mathbf{A}=\mathbf{A}^{\top}\)</span>, i.e., <span class="math inline">\(a_{ij}=a_{ji}\)</span> for all <span class="math inline">\(i,j\)</span>. In the <strong>network</strong> model, the mutual relation on a set of <span class="math inline">\(n\)</span> people can be represented by an <span class="math inline">\(n\times n\)</span> <a href="ch-vecMat.html#sub:matrix">symmetric matrix</a>. For three elements <span class="math inline">\(n=3\)</span>, there exists <span class="math inline">\(6\)</span> possible structures of the (mutual) relations. Figure <a href="ch-vecMat.html#fig:adjMatrix">10.12</a> shows four of them, and their associated <strong>adjacency matrices</strong>.<label for="tufte-sn-177" class="margin-toggle sidenote-number">177</label><input type="checkbox" id="tufte-sn-177" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">177</span>  If we extend <span class="math inline">\(n\)</span> to four and five, the possible relations will grow (exponentially). Figure <a href="ch-vecMat.html#fig:adjMatrix">10.12</a> also shows some possible patterns and their <strong>adjacency matrices</strong>.</span></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:pageRank"></span>
<img src="fig/Part3/pageRank.png" alt="Six pages" width="100%"><!--
<p class="caption marginnote">-->Figure 10.13: Six pages<!--</p>-->
<!--</div>--></span>
</p>
<p>For a <strong>directed network</strong>, the edge <span class="math inline">\(\{\mbox{red},\mbox{green}\}\)</span> does not mean <span class="math inline">\(\{\mbox{green},\mbox{red}\}\)</span>. One has to figure out the valid direction of the relation. One popular directed network model, called PageRank algorithm, was developed by google for matching the desired web pages in a search engine. Suppose a web of six pages represented as in figure <a href="ch-vecMat.html#fig:pageRank">10.13</a> with pages as <strong>vertices</strong> and links from one page to another (arrows) as <strong>directed edges</strong>. To rank the importance of a page, we could count the incoming links (called backlinks) of each page and then rank pages according to the counting score. Let the score for page <span class="math inline">\(i\)</span> be <span class="math inline">\(x_{i}\)</span>. One can define <span class="math inline">\(x_{i}\)</span> as <span class="math inline">\(x_{i}=\sum_{j\sim i}x_{j}/\mbox{deg}(j)\)</span> where <span class="math inline">\(j\sim i\)</span> and <span class="math inline">\(\mbox{deg}(j)\)</span> measures the total number of outgoing links on the page <span class="math inline">\(j\)</span>.<label for="tufte-sn-178" class="margin-toggle sidenote-number">178</label><input type="checkbox" id="tufte-sn-178" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">178</span> The equation says each page divides its one unit of influence among all pages to which it links, so that no page has more influence to distribute than any other.</span> From figure <a href="ch-vecMat.html#fig:pageRank">10.13</a>, we have the <strong>adjacency matrix</strong> as well as the matrix for the scores
<span class="math display">\[\mathbf{A}=\left[\begin{array}{cccccc}
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0
\end{array}\right],\quad\mathbf{P}=\left[\begin{array}{cccccc}
0 &amp; 0 &amp; 1/3 &amp; 0 &amp; 0 &amp; 0\\
1/2 &amp; 0 &amp; 1/3 &amp; 0 &amp; 0 &amp; 0\\
1/2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1/3 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0
\end{array}\right]\]</span>
where <span class="math inline">\(\mathbf{P}\)</span> is to divide each column <span class="math inline">\(j\)</span> of <span class="math inline">\(\mathbf{A}\)</span> by the <strong>degree</strong> of this vertex <span class="math inline">\(j\)</span>. The system of the page scores is given by <span class="math display">\[\mathbf{x}=\mathbf{P}\mathbf{x}\]</span> which tells us is that the desired ranking vector <span class="math inline">\(\mathbf{x}\)</span> is really a <strong>stationary vector</strong> for the matrix <span class="math inline">\(\mathbf{P}\)</span>. Since <span class="math inline">\(\mathbf{x}\)</span> appears as the input and the output of this linear system, the system is referred to a <em>linear simultaneous system</em>.</p>
<p>The <strong>linear simultaneous system</strong> is a fundamental tool for modeling input-output relations. Let’s assume that in an economy with <span class="math inline">\(n\)</span> different industrial sectors, the total production output of sector <span class="math inline">\(i\)</span> is <span class="math inline">\(x_{i}\)</span> for <span class="math inline">\(i=1,\dots,n\)</span>. The output of each sector can flow to other sectors for supporting the production or it can flow to the consumers. Let the demand for sector <span class="math inline">\(i\)</span> be <span class="math inline">\(d_{i}\)</span>. Let the <em>input-output matrix</em> <span class="math inline">\(\mathbf{A}\)</span> characterize the flows between the sectors. For example, the output <span class="math inline">\(x_{j}\)</span> in the sector <span class="math inline">\(j\)</span> requires <span class="math inline">\(a_{ij}x_{j}\)</span> unit input from the sector <span class="math inline">\(i\)</span>. The full expression of this economy becomes <span class="math display">\[\mathbf{x}=\mathbf{A}\mathbf{x}+\mathbf{d}\]</span>
which means that the total production of each sector matches the demand of this sector plus the total amount required to support production in this sector. The model is known as (Leontief’s) <em>input-output model</em>.</p>
</div>
<div id="sub:symbolism" class="section level2">
<h2>
<span class="header-section-number">10.5</span> Miscellaneous: Mythology and Symbolism</h2>
<p>As we have seen that the additions of vectors and matrices are somehow different from the additions of numbers. The “+” sign attached to these addition is merely a <strong>symbol</strong>. When we endow this symbol to the general meaning (the <a href="#sub:vector">four axioms</a>), this <strong>symbol</strong> is equipped with new significance. And the new significance does include the specific meaning of the addition of integers.</p>
<p><em>Symbolization</em> belongs to abstraction - a genral reference to the realm of reality from which the form (in a philosophical sense) is abstracted, a reflection of the laws of that realm, a “logical picture” into which all instances must fit.</p>
<p>Symbols have their roles in religious rituals and in scientific explorations. The fundamental perspective of most <strong>religions</strong> is the notion of embodying a timeless truth that was derived either from a divine source or from some insight into an unchanging reality. It may be a challenge for religions to admit that something absolutely basic to the world can be changed as the evolutionary meanings of symbols. However, in response to the transitions (and perhaps the associated threats), religious did use symbols to expand their explainable and inferential power. Ancient Chinese founded their religious or eastern philosophical system by divinatory symbols representing the changes of nature. Five elements (earth, metal, water, wood and fire), for example, was a skeleton in the Chinese belief. With such belief, the phenomena in astrology, medicine, military strategy, etc., were successively interpreted on the basis of these five symbols. Gradually, the five elements based interpretations reinforce Chinese belief of the world as such.</p>
<p>By <strong>symbols</strong>, one can transfer experiences that no other medium can adequately express. Aesthetic attraction and mysterious fear are probably the first manifestations of the <strong>symbolization</strong> in which people perceived some peculiar tendencies of visualizing the reality, and by which they were able to concretize some abstract concepts and projected them into the reality. When there were no clear <a href="sub-inferknow.html#determinism">casuality</a> among the features, people were intended to attach some distant features to the existing symbolic meanings, and the meanings later would react upon the people. It creates a closed loop to <a href="ch-vecMat.html#sub:linearSys">control</a> the humans cognitions towards the unknown casuality and gives the initial consciousness of the scientific reasoning.</p>
<p>Although symbols have rich content and hidden natures, they are vague and lack of definitive explainations.<label for="tufte-sn-179" class="margin-toggle sidenote-number">179</label><input type="checkbox" id="tufte-sn-179" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">179</span> Can the indirect ways express more absolute truths? Can abstract symbols compress more information than concrete characters? Thinking these questions systematically may stimulate your spiritual resonance of symbolization.</span> The evolutionary paths of symbolization is diverse. When Chinese attached their worldview firmly with the fixed symbolic system made of five elements and eight trigrams, and completed a self-consistent reasoning logic as “theory of everything;” on the contrary, the <strong>symbolization</strong> in western world was able to detach from the preconceived areas, explored and evolved into a much deeper field, and created a more abstract vision of the world. From late 19th century to early 20th century, the core period of the second industrial revolution as well as the period of reflection and self-examination with the rise of communism and social Darwinism on the eve of the WWI, the <em>symbolism</em> declared to abandon the direct reproduction of reality, and to focus on the multifaceted synthesis of the phenomena.<label for="tufte-sn-180" class="margin-toggle sidenote-number">180</label><input type="checkbox" id="tufte-sn-180" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">180</span> In arts and literature, <strong>symbolism</strong> foucses on the brimming situations with emotional content and expressive energy. One representative work of this style can be found in Les Fleurs du mal, a volume of poetry written by Charles Baudelaire.</span> The movement somehow successfully implied various thoughts through polysemous symbols.</p>
<div class="solution">
<p class="solution-begin">
Miscellaneous <span id="sol-start-36" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-36', 'sol-start-36')"></span>
</p>
<div id="sol-body-36" class="solution-body" style="display: none;">
<p><span class="newthought">Mutual overcoming and mutual generation </span></p>
<p>The interactions and (dynamical) relationships amongst five elements are often described by the mutual overcoming (or counteract graph) and mutual generation (or reinforce graph). These relationships can be easily expressed by the symbols of <a href="ch-vecMat.html#sub:matrix">matrices</a>. By matrices, they form two categories of networks. Let’s label earth as <span class="math inline">\(1\)</span>, metal <span class="math inline">\(2\)</span>, water <span class="math inline">\(3\)</span>, wood <span class="math inline">\(4\)</span>, fire <span class="math inline">\(5\)</span>. For the counteract graph, the adjaency matrix <span class="math inline">\([a_{ij}]_{5\times5}\)</span> has <span class="math inline">\(a_{12}=1\)</span>, <span class="math inline">\(a_{23}=1\)</span>, <span class="math inline">\(a_{34}=1\)</span>, <span class="math inline">\(a_{45}=1\)</span>, <span class="math inline">\(a_{51}=1\)</span> and zero elsewhere. Similarly, the reinforce graph has <span class="math inline">\([b_{ij}]_{5\times5}\)</span> with <span class="math inline">\(b_{13}=1\)</span>, <span class="math inline">\(b_{24}=1\)</span>, <span class="math inline">\(b_{35}=1\)</span>, <span class="math inline">\(b_{41}=1\)</span>, <span class="math inline">\(b_{52}=1\)</span>, and zero elsewhere. So we have the matrix expressions for the mutual overcoming and mutual generation: <span class="math display">\[\mathbf{A}=\left[\begin{array}{ccccc}
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0
\end{array}\right] \, , \, \mathbf{B}=\left[\begin{array}{ccccc}
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0
\end{array}\right].\]</span></p>
<p><span class="newthought">L’Homme et la mer </span></p>
<p>L’Homme et la mer is a poem from Les Fleurs du mal.</p>
<blockquote>
<p>L’Homme et la mer</p>
</blockquote>
<blockquote>

</blockquote>
<blockquote>
<p>Homme libre, toujours tu chériras la mer!
La mer est ton miroir; tu contemples ton âme
Dans le déroulement infini de sa lame,
Et ton esprit n’est pas un gouffre moins amer.</p>
</blockquote>
<blockquote>

</blockquote>
<blockquote>
<p>Tu te plais à plonger au sein de ton image;
Tu l’embrasses des yeux et des bras, et ton coeur
Se distrait quelquefois de sa propre rumeur
Au bruit de cette plainte indomptable et sauvage.</p>
</blockquote>
<blockquote>

</blockquote>
<blockquote>
<p>Vous êtes tous les deux ténébreux et discrets:
Homme, nul n’a sondé le fond de tes abîmes;
Ô mer, nul ne connaît tes richesses intimes,
Tant vous êtes jaloux de garder vos secrets!</p>
</blockquote>
<blockquote>

</blockquote>
<blockquote>
<p>Et cependant voilà des siècles innombrables
Que vous vous combattez sans pitié ni remords,
Tellement vous aimez le carnage et la mort,
Ô lutteurs éternels, ô frères implacables!</p>
</blockquote>
<blockquote>

</blockquote>
<blockquote>
<p>Man and the Sea</p>
</blockquote>
<blockquote>

</blockquote>
<blockquote>
<p>Free man, you will always cherish the sea!
The sea is your mirror; you contemplate your soul
In the infinite unrolling of its billows;
Your mind is an abyss that is no less bitter.</p>
</blockquote>
<blockquote>

</blockquote>
<blockquote>
<p>You like to plunge into the bosom of your image;
You embrace it with eyes and arms, and your heart
Is distracted at times from its own clamoring
By the sound of this plaint, wild and untamable.</p>
</blockquote>
<blockquote>

</blockquote>
<blockquote>
<p>Both of you are gloomy and reticent:
Man, no one has sounded the depths of your being;
O Sea, no person knows your most hidden riches,
So zealously do you keep your secrets!</p>
</blockquote>
<blockquote>

</blockquote>
<blockquote>
<p>Yet for countless ages you have fought each other
Without pity, without remorse,
So fiercely do you love carnage and death,
O eternal fighters, implacable brothers!</p>
</blockquote>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
</div>
</div></body></html>

<p style="text-align: center;">
<a href="part-iii-emergence-of-abstract-interactions.html"><button class="btn btn-default">Previous</button></a>
<a href="ch-MatComp.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2021-06-01
</p>
</div>
</div>



</body>
</html>
